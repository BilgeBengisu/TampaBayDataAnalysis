{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc626fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             username                                        description  \\\n",
      "0       ChungSunPark4                                      attempt me!!!   \n",
      "1        LucilaQuanti  Me gusta la gente con sentido del humor, alegr...   \n",
      "2       patdefranchis  I love people with a large dose of humor & a r...   \n",
      "3       saravastiares  Have courage & Be Kind. Where there is kindnes...   \n",
      "4      TheShogunGamer  Video Game Extraordinaire üéÆ Polyamorous ‚ù§ Disa...   \n",
      "...               ...                                                ...   \n",
      "3493   ChrisFischer07                                                NaN   \n",
      "3494       ryantpa813  Eight One Three Sports\\nBolts, Bucs, Rays, Soo...   \n",
      "3495  TueNiteRockStar                                                NaN   \n",
      "3496     purpletang99  i tweet my opinions only. yes there are except...   \n",
      "3497       firefly909  Unapologetic bleeding heart liberal. Hate liar...   \n",
      "\n",
      "                                 description_lemmatized  \n",
      "0                      ['attempt', 'me', '!', '!', '!']  \n",
      "1     ['me', 'gusta', 'la', 'gente', 'con', 'sentido...  \n",
      "2     ['i', 'love', 'people', 'with', 'a', 'large', ...  \n",
      "3     ['have', 'courage', '&', 'be', 'kind', '.', 'w...  \n",
      "4     ['video', 'game', 'extraordinaire', 'üéÆ', 'poly...  \n",
      "...                                                 ...  \n",
      "3493                                                     \n",
      "3494  ['eight', 'one', 'three', 'sport', 'bolt', ','...  \n",
      "3495                                                     \n",
      "3496  ['i', 'tweet', 'my', 'opinion', 'only', '.', '...  \n",
      "3497  ['unapologetic', 'bleed', 'heart', 'liberal', ...  \n",
      "\n",
      "[3498 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_predict, train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import json\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pickle\n",
    "\n",
    "tag_map = defaultdict(lambda: wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV\n",
    "tag_map['AS'] = wn.ADJ_SAT\n",
    "\n",
    "# filepath = \"finalized_8K_accounts.csv\"\n",
    "# filepath = \"UNLABELED_accounts_emojis_replaced.csv\"\n",
    "filepath = \"Spill_Accounts_To_Be_Labeled.csv\"\n",
    "hand_label = \"hand.label\"\n",
    "government = \"gov\"\n",
    "academia = \"acad\"\n",
    "tourBiz = \"tourbiz\"\n",
    "\n",
    "df = pd.read_csv(filepath)\n",
    "\n",
    "# df = df[((df[hand_label] == 'media') | (df[hand_label] == tourBiz) |(df[hand_label] == academia) | (df[hand_label] == government) | (\n",
    "#        df[hand_label] == 'other'))]\n",
    "\n",
    "df = df[['username', 'description']]  # keep only relevant columns\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words_not_changed = ['media']\n",
    "\n",
    "\n",
    "def preprocessing(row):\n",
    "    if str(row) == \"nan\":\n",
    "        lemma = \"\"\n",
    "    else:\n",
    "        row = str(row).lower()\n",
    "        row = word_tokenize(row)  # tokenize\n",
    "        lemma = [lemmatizer.lemmatize(token, tag_map[tag[0]]) if token not in words_not_changed else token for\n",
    "                 token, tag in pos_tag(row)]  # lemmatization, depending on part-of-speech\n",
    "        lemma = [\"\" if re.search(r'\\b[0-9]+\\b\\s*', lem) else lem for lem in lemma]  # removing\n",
    "    return str(lemma)\n",
    "\n",
    "\n",
    "df['description_lemmatized'] = df['description'].apply(preprocessing)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b4d3f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3498, 3)\n",
      "(2908, 3)\n"
     ]
    }
   ],
   "source": [
    "# all the empty descriptions\n",
    "print(df.shape)\n",
    "print(df[df['description_lemmatized'] != \"\"].shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57a23e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(590, 3)\n",
      "(2908, 3)\n"
     ]
    }
   ],
   "source": [
    "# Remove all the empty descriptions\n",
    "empty_rows = df[df['description_lemmatized'] == \"\"]\n",
    "print(empty_rows.shape)\n",
    "df = df[df['description_lemmatized'] != \"\"]\n",
    "print(df.shape)\n",
    "#df[hand_label]\n",
    "#print(df.shape)\n",
    "#df[df['description_lemmatized'] != \"\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f7612c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-indexing the remaining observations\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "2af8cfdb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Tensor' from 'torch' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[145], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m SentenceTransformer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall-MiniLM-L6-v2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# print(type(df[['description_lemmatized']]))\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sentence_transformers/__init__.py:7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimportlib\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcross_encoder\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mCrossEncoder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrossEncoder\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ParallelSentencesDataset, SentencesDataset\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mLoggingHandler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LoggingHandler\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sentence_transformers/cross_encoder/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mCrossEncoder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrossEncoder\n\u001b[1;32m      3\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCrossEncoder\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tensor, nn\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Optimizer\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Tensor' from 'torch' (unknown location)"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# print(type(df[['description_lemmatized']]))\n",
    "embeddings = model.encode(df['description'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13dfb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = 'SVM_BOW_unweighted_enhanced_model.pickle'\n",
    "filename = 'SVM_BERT_unweighted_enhanced_model_full(1, 2).pickle'\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "X_test = embeddings\n",
    "\n",
    "bag_of_words_y_pred_test = loaded_model.predict(X_test)\n",
    "\n",
    "bag_of_words_y_pred_test\n",
    "\n",
    "pred_prob = loaded_model.predict_proba(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1d7478",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_prob\n",
    "bag_of_words_y_pred_test\n",
    "pd.concat([pd.DataFrame(bag_of_words_y_pred_test), pd.DataFrame(pred_prob)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae85ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_prob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd3b9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_prob_df = pd.DataFrame(pred_prob, columns = ['acad_prob','gov_prob','media_prob','other_prob', 'tourbiz_prob'])\n",
    "\n",
    "bag_of_words_y_pred_test.size\n",
    "\n",
    "df['hand.label_simplified'] = bag_of_words_y_pred_test\n",
    "#df = df.drop(columns=['description_lemmatized'])\n",
    "df1 = pd.concat([df, pred_prob_df], axis=1)\n",
    "#df1 = pd.DataFrame(my_array, columns = ['acad_prob','gov_prob','media_prob','other_prob', 'tourbiz_prob'])\n",
    "\n",
    "df1\n",
    "df1.shape\n",
    "#pred_prob_df.shape\n",
    "#len(bag_of_words_y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "8b0141ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[151], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df1\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df1' is not defined"
     ]
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5f0ec6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv(r'SVM_BERT_unweighted_UNLABELED_PREDICTED_accounts_W_PROBABILITIES_emojis_unchanged.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1ce37dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "spill_labeled_accs = pd.read_csv(\"Spill_Labeled.csv\")\n",
    "#spill_labeled_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4ea2422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26172\n",
      "hand.label_simplified\n",
      "other      2680\n",
      "media       162\n",
      "acad         62\n",
      "tourbiz       3\n",
      "gov           1\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "hand.label_simplified\n",
       "other      0.921596\n",
       "media      0.055708\n",
       "acad       0.021320\n",
       "tourbiz    0.001032\n",
       "gov        0.000344\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(spill_labeled_accs.size)\n",
    "\n",
    "print(spill_labeled_accs[\"hand.label_simplified\"].value_counts())\n",
    "\n",
    "spill_labeled_accs[\"hand.label_simplified\"].value_counts(\"row\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38216109",
   "metadata": {},
   "source": [
    "# Relabeling \"other\" rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "90e98606-aa67-431b-b022-a4de1008c18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the rows labeled as other\n",
    "mask = spill_labeled_accs[\"hand.label_simplified\"] == 'other'\n",
    "\n",
    "\n",
    "row_list = []\n",
    "# Check if any of the probabilities (except 'other_prob') are greater than 0.3\n",
    "for index, row in spill_labeled_accs[mask].iterrows():\n",
    "    for col in ['acad_prob', 'gov_prob', 'media_prob', 'other_prob', 'tourbiz_prob']:\n",
    "        if col != 'other_prob' and row[col] > 0.3:\n",
    "            # saving the columns to review\n",
    "            row_list.append(row)\n",
    "            # Update the prediction column to the column name where the probability is higher than 0.3\n",
    "            #spill_labeled_accs.at[index, \"hand.label_simplified\"] = col.replace('_prob', '')\n",
    "rows_for_review = pd.DataFrame(row_list)\n",
    "rows_for_review = rows_for_review.drop_duplicates()\n",
    "#print(rows_for_review)\n",
    "\n",
    "rows_for_review.to_csv(\"Accounts_To_Relabel.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7e70eb64-18a4-4ac1-9b23-4c02e05c8006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hand.label_simplified\n",
      "other      2680\n",
      "media       162\n",
      "acad         62\n",
      "tourbiz       3\n",
      "gov           1\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>description</th>\n",
       "      <th>description_lemmatized</th>\n",
       "      <th>hand.label_simplified</th>\n",
       "      <th>acad_prob</th>\n",
       "      <th>gov_prob</th>\n",
       "      <th>media_prob</th>\n",
       "      <th>other_prob</th>\n",
       "      <th>tourbiz_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ChungSunPark4</td>\n",
       "      <td>attempt me!!!</td>\n",
       "      <td>['attempt', 'me', '!', '!', '!']</td>\n",
       "      <td>other</td>\n",
       "      <td>0.000776</td>\n",
       "      <td>0.000603</td>\n",
       "      <td>0.004676</td>\n",
       "      <td>0.993783</td>\n",
       "      <td>0.000162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LucilaQuanti</td>\n",
       "      <td>Me gusta la gente con sentido del humor, alegr...</td>\n",
       "      <td>['me', 'gusta', 'la', 'gente', 'con', 'sentido...</td>\n",
       "      <td>other</td>\n",
       "      <td>0.009851</td>\n",
       "      <td>0.001305</td>\n",
       "      <td>0.006343</td>\n",
       "      <td>0.981409</td>\n",
       "      <td>0.001092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>patdefranchis</td>\n",
       "      <td>I love people with a large dose of humor &amp; a r...</td>\n",
       "      <td>['i', 'love', 'people', 'with', 'a', 'large', ...</td>\n",
       "      <td>other</td>\n",
       "      <td>0.003543</td>\n",
       "      <td>0.000746</td>\n",
       "      <td>0.004899</td>\n",
       "      <td>0.990260</td>\n",
       "      <td>0.000552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>saravastiares</td>\n",
       "      <td>Have courage &amp; Be Kind. Where there is kindnes...</td>\n",
       "      <td>['have', 'courage', '&amp;', 'be', 'kind', '.', 'w...</td>\n",
       "      <td>other</td>\n",
       "      <td>0.003940</td>\n",
       "      <td>0.000537</td>\n",
       "      <td>0.007693</td>\n",
       "      <td>0.985011</td>\n",
       "      <td>0.002819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TheShogunGamer</td>\n",
       "      <td>Video Game Extraordinaire üéÆ Polyamorous ‚ù§ Disa...</td>\n",
       "      <td>['video', 'game', 'extraordinaire', 'üéÆ', 'poly...</td>\n",
       "      <td>other</td>\n",
       "      <td>0.000925</td>\n",
       "      <td>0.000598</td>\n",
       "      <td>0.003607</td>\n",
       "      <td>0.994229</td>\n",
       "      <td>0.000641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2903</th>\n",
       "      <td>meevans59</td>\n",
       "      <td>Retired Department of Defense civilian employe...</td>\n",
       "      <td>['retired', 'department', 'of', 'defense', 'ci...</td>\n",
       "      <td>other</td>\n",
       "      <td>0.001988</td>\n",
       "      <td>0.004162</td>\n",
       "      <td>0.002987</td>\n",
       "      <td>0.990808</td>\n",
       "      <td>0.000055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2904</th>\n",
       "      <td>dvdhnz</td>\n",
       "      <td>Techie by trade, recalcitrant by design, socia...</td>\n",
       "      <td>['techie', 'by', 'trade', ',', 'recalcitrant',...</td>\n",
       "      <td>other</td>\n",
       "      <td>0.015865</td>\n",
       "      <td>0.002104</td>\n",
       "      <td>0.001636</td>\n",
       "      <td>0.978123</td>\n",
       "      <td>0.002272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2905</th>\n",
       "      <td>ryantpa813</td>\n",
       "      <td>Eight One Three Sports\\nBolts, Bucs, Rays, Soo...</td>\n",
       "      <td>['eight', 'one', 'three', 'sport', 'bolt', ','...</td>\n",
       "      <td>other</td>\n",
       "      <td>0.000745</td>\n",
       "      <td>0.001396</td>\n",
       "      <td>0.033677</td>\n",
       "      <td>0.962982</td>\n",
       "      <td>0.001200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2906</th>\n",
       "      <td>purpletang99</td>\n",
       "      <td>i tweet my opinions only. yes there are except...</td>\n",
       "      <td>['i', 'tweet', 'my', 'opinion', 'only', '.', '...</td>\n",
       "      <td>other</td>\n",
       "      <td>0.003356</td>\n",
       "      <td>0.000360</td>\n",
       "      <td>0.076140</td>\n",
       "      <td>0.919635</td>\n",
       "      <td>0.000509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2907</th>\n",
       "      <td>firefly909</td>\n",
       "      <td>Unapologetic bleeding heart liberal. Hate liar...</td>\n",
       "      <td>['unapologetic', 'bleed', 'heart', 'liberal', ...</td>\n",
       "      <td>other</td>\n",
       "      <td>0.000927</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000698</td>\n",
       "      <td>0.998056</td>\n",
       "      <td>0.000247</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2908 rows √ó 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            username                                        description  \\\n",
       "0      ChungSunPark4                                      attempt me!!!   \n",
       "1       LucilaQuanti  Me gusta la gente con sentido del humor, alegr...   \n",
       "2      patdefranchis  I love people with a large dose of humor & a r...   \n",
       "3      saravastiares  Have courage & Be Kind. Where there is kindnes...   \n",
       "4     TheShogunGamer  Video Game Extraordinaire üéÆ Polyamorous ‚ù§ Disa...   \n",
       "...              ...                                                ...   \n",
       "2903       meevans59  Retired Department of Defense civilian employe...   \n",
       "2904          dvdhnz  Techie by trade, recalcitrant by design, socia...   \n",
       "2905      ryantpa813  Eight One Three Sports\\nBolts, Bucs, Rays, Soo...   \n",
       "2906    purpletang99  i tweet my opinions only. yes there are except...   \n",
       "2907      firefly909  Unapologetic bleeding heart liberal. Hate liar...   \n",
       "\n",
       "                                 description_lemmatized hand.label_simplified  \\\n",
       "0                      ['attempt', 'me', '!', '!', '!']                 other   \n",
       "1     ['me', 'gusta', 'la', 'gente', 'con', 'sentido...                 other   \n",
       "2     ['i', 'love', 'people', 'with', 'a', 'large', ...                 other   \n",
       "3     ['have', 'courage', '&', 'be', 'kind', '.', 'w...                 other   \n",
       "4     ['video', 'game', 'extraordinaire', 'üéÆ', 'poly...                 other   \n",
       "...                                                 ...                   ...   \n",
       "2903  ['retired', 'department', 'of', 'defense', 'ci...                 other   \n",
       "2904  ['techie', 'by', 'trade', ',', 'recalcitrant',...                 other   \n",
       "2905  ['eight', 'one', 'three', 'sport', 'bolt', ','...                 other   \n",
       "2906  ['i', 'tweet', 'my', 'opinion', 'only', '.', '...                 other   \n",
       "2907  ['unapologetic', 'bleed', 'heart', 'liberal', ...                 other   \n",
       "\n",
       "      acad_prob  gov_prob  media_prob  other_prob  tourbiz_prob  \n",
       "0      0.000776  0.000603    0.004676    0.993783      0.000162  \n",
       "1      0.009851  0.001305    0.006343    0.981409      0.001092  \n",
       "2      0.003543  0.000746    0.004899    0.990260      0.000552  \n",
       "3      0.003940  0.000537    0.007693    0.985011      0.002819  \n",
       "4      0.000925  0.000598    0.003607    0.994229      0.000641  \n",
       "...         ...       ...         ...         ...           ...  \n",
       "2903   0.001988  0.004162    0.002987    0.990808      0.000055  \n",
       "2904   0.015865  0.002104    0.001636    0.978123      0.002272  \n",
       "2905   0.000745  0.001396    0.033677    0.962982      0.001200  \n",
       "2906   0.003356  0.000360    0.076140    0.919635      0.000509  \n",
       "2907   0.000927  0.000073    0.000698    0.998056      0.000247  \n",
       "\n",
       "[2908 rows x 9 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(spill_labeled_accs[\"hand.label_simplified\"].value_counts())\n",
    "\n",
    "spill_labeled_accs[\"hand.label_simplified\"].value_counts(\"row\")\n",
    "spill_labeled_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b534871b-8605-4fb8-ba09-4e9c07c4f346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hand.label_simplified\n",
       "other      2606\n",
       "media       162\n",
       "acad         62\n",
       "other        36\n",
       "media        17\n",
       "acad         14\n",
       "gov           5\n",
       "tourbiz       3\n",
       "tourbiz       2\n",
       "gov           1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## replacing some rows with their manually labeled values\n",
    "\n",
    "relabeled = pd.read_csv(\"Final_Account_Relabeling.csv\")\n",
    "mask = spill_labeled_accs[\"username\"].isin(relabeled[\"username\"])\n",
    "# Update columns in df1 with corresponding values from df2 where usernames match\n",
    "for index, row in spill_labeled_accs[mask].iterrows():\n",
    "    relabel_row = relabeled[relabeled['username'] == row['username']]\n",
    "    spill_labeled_accs.loc[index, 'hand.label_simplified'] = relabel_row['hand.label_simplified'].values\n",
    "spill_labeled_accs[\"hand.label_simplified\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "65319d29-2b4b-4cf7-a9ac-941aa25480d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3498, 9)\n",
      "             username                                        description  \\\n",
      "0             004nino  Retrait√©.√âgalit√©,fraternit√©,justice,libert√©,na...   \n",
      "1              00a03d  Mom of one, grandma of two, sister of six, aun...   \n",
      "2     05adamlover0129  LOVE Adam Lambert for life!!! Also love Miami ...   \n",
      "3     0Plongstocking2  Not a traitor. Traveler. Liker of food. Orchid...   \n",
      "4             0Thessa  üíö... sch√∂nheit\\nist die natur! die kunst ist u...   \n",
      "...               ...                                                ...   \n",
      "3493   zoeycarmicheal  The Univ. of Alabama Alumni. Former Gymnast an...   \n",
      "3494           zpleat  Research at Media Matters for America. All twe...   \n",
      "3495         zshahan3  Human (maybe), writer + chief editor + CEO @Cl...   \n",
      "3496          zul1732  You can't see people for what they are when yo...   \n",
      "3497        zyiteblog  E-Scootersworld is global Power transport reta...   \n",
      "\n",
      "                                 description_lemmatized hand.label_simplified  \\\n",
      "0     ['retrait√©.√©galit√©', ',', 'fraternit√©', ',', '...                 other   \n",
      "1     ['mom', 'of', 'one', ',', 'grandma', 'of', 'tw...                 other   \n",
      "2     ['love', 'adam', 'lambert', 'for', 'life', '!'...                 other   \n",
      "3     ['not', 'a', 'traitor', '.', 'traveler', '.', ...                 other   \n",
      "4     ['üíö', '...', 'sch√∂nheit', 'ist', 'die', 'natur...                 other   \n",
      "...                                                 ...                   ...   \n",
      "3493  ['the', 'univ', '.', 'of', 'alabama', 'alumnus...                 other   \n",
      "3494  ['research', 'at', 'media', 'matter', 'for', '...                 other   \n",
      "3495  ['human', '(', 'maybe', ')', ',', 'writer', '+...                 media   \n",
      "3496  ['you', 'ca', \"n't\", 'see', 'people', 'for', '...                 other   \n",
      "3497  ['e-scootersworld', 'be', 'global', 'power', '...                 other   \n",
      "\n",
      "      acad_prob  gov_prob  media_prob  other_prob  tourbiz_prob  \n",
      "0      0.017303  0.002636    0.010321    0.969045      0.000695  \n",
      "1      0.008145  0.002770    0.011733    0.977149      0.000203  \n",
      "2      0.005680  0.000770    0.202065    0.788418      0.003066  \n",
      "3      0.006012  0.005455    0.001879    0.980456      0.006198  \n",
      "4      0.014347  0.017324    0.018455    0.948339      0.001535  \n",
      "...         ...       ...         ...         ...           ...  \n",
      "3493   0.003370  0.003790    0.041372    0.948383      0.003084  \n",
      "3494   0.031496  0.002862    0.050988    0.914259      0.000394  \n",
      "3495   0.004032  0.000303    0.693154    0.302204      0.000308  \n",
      "3496   0.005312  0.000945    0.001609    0.986842      0.005293  \n",
      "3497   0.005406  0.005807    0.019533    0.938352      0.030902  \n",
      "\n",
      "[3498 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "spill_accs_merged = pd.merge(spill_labeled_accs, empty_rows, how=\"outer\")\n",
    "\n",
    "print(spill_accs_merged.shape)\n",
    "print(spill_accs_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0c61f255-89d1-455c-b08b-cf584df4fe9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              username                                        description  \\\n",
      "0                  CNN  It‚Äôs our job to #GoThere & tell the most diffi...   \n",
      "1               NatGeo  Taking our understanding and awareness of the ...   \n",
      "2              FoxNews  Follow America's #1 cable news network, delive...   \n",
      "3       washingtonpost                         Democracy Dies in Darkness   \n",
      "4                  ABC  The only official ABC News Twitter account. Do...   \n",
      "...                ...                                                ...   \n",
      "29128  EvergreenZephyr  Wichita, Kansas, United (sic) States. Parody a...   \n",
      "29129         johntfox  Madeleine & Marin's Dad | Gin Enthusiast | Twe...   \n",
      "29130         SeGreene  Cranky former nurse and current plant patholog...   \n",
      "29131      CherylLasse  Passionate about the environment, science and ...   \n",
      "29132          jen_pic  üö´socialism. Pay your debts, ALL OF THEM! Nothi...   \n",
      "\n",
      "       Label Label.Type  \n",
      "0      media       Hand  \n",
      "1      media       Hand  \n",
      "2      media       Hand  \n",
      "3      media       Hand  \n",
      "4      media       Hand  \n",
      "...      ...        ...  \n",
      "29128  other    Predict  \n",
      "29129  other    Predict  \n",
      "29130  other    Predict  \n",
      "29131  other    Predict  \n",
      "29132  other    Predict  \n",
      "\n",
      "[29133 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# acccounts labeled during Red-Tide research\n",
    "prev_accs = pd.read_csv(\"Final_Account_Labels_for_Dashboard.csv\")\n",
    "print(prev_accs)\n",
    "accounts_merged = pd.concat([spill_accs_merged, prev_accs], ignore_index=True)\n",
    "\n",
    "accounts_merged.to_csv(\"ALL_Labeled_Accounts_Spill&RedTide.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "28303242-c700-4917-b0b4-4f62a86581cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hand.label_simplified\n",
      "other      247\n",
      "media       31\n",
      "acad        19\n",
      "media        7\n",
      "acad         3\n",
      "other        2\n",
      "gov          1\n",
      "tourbiz      1\n",
      "Name: count, dtype: int64\n",
      "hand.label_simplified\n",
      "other      0.794212\n",
      "media      0.099678\n",
      "acad       0.061093\n",
      "media      0.022508\n",
      "acad       0.009646\n",
      "other      0.006431\n",
      "gov        0.003215\n",
      "tourbiz    0.003215\n",
      "Name: proportion, dtype: float64\n",
      "hand.label_simplified\n",
      "other      207\n",
      "media        9\n",
      "acad         4\n",
      "other        3\n",
      "gov          3\n",
      "media        2\n",
      "tourbiz      1\n",
      "gov          1\n",
      "Name: count, dtype: int64\n",
      "hand.label_simplified\n",
      "other      0.900000\n",
      "media      0.039130\n",
      "acad       0.017391\n",
      "other      0.013043\n",
      "gov        0.013043\n",
      "media      0.008696\n",
      "tourbiz    0.004348\n",
      "gov        0.004348\n",
      "Name: proportion, dtype: float64\n",
      "hand.label_simplified\n",
      "other      2179\n",
      "media       127\n",
      "acad         39\n",
      "other        31\n",
      "acad         11\n",
      "media         9\n",
      "tourbiz       2\n",
      "tourbiz       1\n",
      "gov           1\n",
      "gov           1\n",
      "Name: count, dtype: int64\n",
      "hand.label_simplified\n",
      "other      0.907539\n",
      "media      0.052895\n",
      "acad       0.016243\n",
      "other      0.012911\n",
      "acad       0.004581\n",
      "media      0.003748\n",
      "tourbiz    0.000833\n",
      "tourbiz    0.000416\n",
      "gov        0.000416\n",
      "gov        0.000416\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# importing the spill files\n",
    "\n",
    "oil = pd.read_csv(\"../spill_data/Cleaned_Files/C_All_Oil.csv\")\n",
    "sewage = pd.read_csv(\"../spill_data/Cleaned_Files/C_All_Sewage.csv\")\n",
    "industrial = pd.read_csv(\"../spill_data/Cleaned_Files/C_All_Industrial.csv\")\n",
    "\n",
    "oil_accs = accounts_merged[accounts_merged[\"username\"].isin(oil[\"username\"])]\n",
    "print(oil_accs[\"hand.label_simplified\"].value_counts())\n",
    "print(oil_accs[\"hand.label_simplified\"].value_counts(\"row\"))\n",
    "\n",
    "sewage_accs = accounts_merged[accounts_merged[\"username\"].isin(sewage[\"username\"])]\n",
    "print(sewage_accs[\"hand.label_simplified\"].value_counts())\n",
    "print(sewage_accs[\"hand.label_simplified\"].value_counts(\"row\"))\n",
    "\n",
    "industrial_accs = accounts_merged[accounts_merged[\"username\"].isin(industrial[\"username\"])]\n",
    "print(industrial_accs[\"hand.label_simplified\"].value_counts())\n",
    "print(industrial_accs[\"hand.label_simplified\"].value_counts(\"row\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c097d0",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9949f866-2efb-4a67-a2c8-e4c4df85e92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf-idf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d54a295b-f43c-4544-b04b-20289b2f4a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Unnamed: 0                                               text  \\\n",
      "0             1  Reposting @divyendhu:\\nOil Spill on the Gulf o...   \n",
      "1             2  RT @hihi0806: Reposting @divyendhu:\\nOil Spill...   \n",
      "2             3  RT @hihi0806: Reposting @divyendhu:\\nOil Spill...   \n",
      "3             4  RT @hihi0806: Reposting @divyendhu:\\nOil Spill...   \n",
      "4             5  RT @hihi0806: Reposting @divyendhu:\\nOil Spill...   \n",
      "..          ...                                                ...   \n",
      "705         615  @1053SS I think this team shouldn‚Äôt even be ov...   \n",
      "706           1  The governor is responsible, but those who cam...   \n",
      "707           2  The governor is responsible, but those who cam...   \n",
      "708           3  The governor is responsible, but those who cam...   \n",
      "709           4  RT @DGRFlorida: The governor is responsible, b...   \n",
      "\n",
      "     possibly_sensitive                   id            author_id  \\\n",
      "0                  True  1079362755352817665             95188073   \n",
      "1                 False  1079363436130136069  1053787947902758918   \n",
      "2                 False  1079462270374215681            842498792   \n",
      "3                 False  1342684372009447424           2241627420   \n",
      "4                 False  1342687535093473286            146645976   \n",
      "..                  ...                  ...                  ...   \n",
      "705               False  1604784465686827009           2419893955   \n",
      "706               False  1033090028497321984           3384569867   \n",
      "707               False  1033091674820423681            175569476   \n",
      "708               False  1033092550393921536   773350091440680960   \n",
      "709               False  1039072225054392320             29393077   \n",
      "\n",
      "         conversation_id  in_reply_to_user_id reply_settings lang  \\\n",
      "0    1079362755352817665                  NaN       everyone   en   \n",
      "1    1079363436130136069                  NaN       everyone   en   \n",
      "2    1079462270374215681                  NaN       everyone   en   \n",
      "3    1342684372009447424                  NaN       everyone   en   \n",
      "4    1342687535093473286                  NaN       everyone   en   \n",
      "..                   ...                  ...            ...  ...   \n",
      "705  1604668490555629568          132734157.0       everyone   en   \n",
      "706  1033090028497321984                  NaN       everyone   en   \n",
      "707  1033091674820423681                  NaN       everyone   en   \n",
      "708  1033092550393921536                  NaN       everyone   en   \n",
      "709  1039072225054392320                  NaN       everyone   en   \n",
      "\n",
      "            created_at.x  ...  \\\n",
      "0    2018-12-30 13:05:09  ...   \n",
      "1    2018-12-30 13:07:51  ...   \n",
      "2    2018-12-30 19:40:35  ...   \n",
      "3    2020-12-26 04:11:14  ...   \n",
      "4    2020-12-26 04:23:48  ...   \n",
      "..                   ...  ...   \n",
      "705  2022-12-19 10:23:26  ...   \n",
      "706  2018-08-24 20:33:51  ...   \n",
      "707  2018-08-24 20:40:23  ...   \n",
      "708  2018-08-24 20:43:52  ...   \n",
      "709  2018-09-10 08:44:58  ...   \n",
      "\n",
      "                                         description_y  \\\n",
      "0    Hi!! ‚òïÔ∏èGood to see you!! üçµPlease be sure to en...   \n",
      "1                                        attempt me!!!   \n",
      "2    Me gusta la gente con sentido del humor, alegr...   \n",
      "3    I love people with a large dose of humor & a r...   \n",
      "4    Have courage & Be Kind. Where there is kindnes...   \n",
      "..                                                 ...   \n",
      "705  The sun never says to earth you owe me for shi...   \n",
      "706  Official Deep Green Resistance Florida! You ca...   \n",
      "707                                                NaN   \n",
      "708  Fighting Pipelines Everywhere! #waterprotector...   \n",
      "709  Up The Ironsü§òSystem Change #NotMeUs NoLISTS #H...   \n",
      "\n",
      "                                description_lemmatized  hand.label_simplified  \\\n",
      "0                                                  NaN                    NaN   \n",
      "1                     ['attempt', 'me', '!', '!', '!']                  other   \n",
      "2    ['me', 'gusta', 'la', 'gente', 'con', 'sentido...                  other   \n",
      "3    ['i', 'love', 'people', 'with', 'a', 'large', ...                  other   \n",
      "4    ['have', 'courage', '&', 'be', 'kind', '.', 'w...                  other   \n",
      "..                                                 ...                    ...   \n",
      "705  ['the', 'sun', 'never', 'say', 'to', 'earth', ...                  other   \n",
      "706                                                NaN                    NaN   \n",
      "707                                                NaN                    NaN   \n",
      "708                                                NaN                    NaN   \n",
      "709                                                NaN                    NaN   \n",
      "\n",
      "    acad_prob  gov_prob  media_prob other_prob tourbiz_prob  Label  Label.Type  \n",
      "0         NaN       NaN         NaN        NaN          NaN  other        Hand  \n",
      "1    0.000776  0.000603    0.004676   0.993783     0.000162    NaN         NaN  \n",
      "2    0.009851  0.001305    0.006343   0.981409     0.001092    NaN         NaN  \n",
      "3    0.003543  0.000746    0.004899   0.990260     0.000552    NaN         NaN  \n",
      "4    0.003940  0.000537    0.007693   0.985011     0.002819    NaN         NaN  \n",
      "..        ...       ...         ...        ...          ...    ...         ...  \n",
      "705  0.002275  0.001970    0.036049   0.957599     0.002107    NaN         NaN  \n",
      "706       NaN       NaN         NaN        NaN          NaN  other        Hand  \n",
      "707       NaN       NaN         NaN        NaN          NaN  other        Hand  \n",
      "708       NaN       NaN         NaN        NaN          NaN  other     Predict  \n",
      "709       NaN       NaN         NaN        NaN          NaN  other     Predict  \n",
      "\n",
      "[710 rows x 42 columns]\n"
     ]
    }
   ],
   "source": [
    "# adding the account labels to the csv with tweets in order to accesss during tf-idf\n",
    "oil_w_acc = pd.merge(oil, accounts_merged, on='username', how='left')\n",
    "print(oil_w_acc)\n",
    "industrial_w_acc = pd.merge(industrial, accounts_merged, on='username', how='left')\n",
    "sewage_w_acc = pd.merge(sewage, accounts_merged, on='username', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "441577bb-0d96-4a2b-a485-2cdb081ea069",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v1/dkrd1tzx2cvd0z9f5pbj8xp00000gn/T/ipykernel_35568/3500104522.py:4: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  oil_w_acc['text_with_display_links'].fillna('', inplace=True)\n",
      "/var/folders/v1/dkrd1tzx2cvd0z9f5pbj8xp00000gn/T/ipykernel_35568/3500104522.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  industrial_w_acc['text_with_display_links'].fillna('', inplace=True)\n",
      "/var/folders/v1/dkrd1tzx2cvd0z9f5pbj8xp00000gn/T/ipykernel_35568/3500104522.py:6: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  sewage_w_acc['text_with_display_links'].fillna('', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# wordcloud analysis based on account type\n",
    "\n",
    "#preprocessing\n",
    "oil_w_acc['text_with_display_links'].fillna('', inplace=True)\n",
    "industrial_w_acc['text_with_display_links'].fillna('', inplace=True)\n",
    "sewage_w_acc['text_with_display_links'].fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b43c3288-343c-4479-8cf8-cff8eccc9206",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Unnamed: 0                                               text  \\\n",
      "6             1  @jamesapisano The Barataria Bay Bottlenose dol...   \n",
      "32            9  Join researchers and responders as they discus...   \n",
      "36           13  RT @NASEM_Gulf: Join researchers and responder...   \n",
      "37           14  RT @NASEM_Gulf: Join researchers and responder...   \n",
      "69           46  RT @usfsp: Associate Professor Heather Judkins...   \n",
      "113          23  As a scientist..we're so frustrated. Politics ...   \n",
      "139          49  RT @RECOVER_GOMRI: #tbt to 2016 Gulf of Mexico...   \n",
      "141          51  RT @RECOVER_GOMRI: #tbt to 2016 Gulf of Mexico...   \n",
      "143          53  RT @RECOVER_GOMRI: #tbt to 2016 Gulf of Mexico...   \n",
      "315         225  We are very pleased to be running a pelagic ec...   \n",
      "316         226  Steve Murawski presented at Tampa Bay's Festiv...   \n",
      "319         229  Join us at the 2020 Gulf of Mexico Oil Spill &...   \n",
      "322         232  RT @NASEM_Gulf: Join us at the 2020 Gulf of Me...   \n",
      "323         233  Join us at the 2020 Gulf of Mexico Oil Spill &...   \n",
      "324         234  RT @NASEM_Gulf: Join us at the 2020 Gulf of Me...   \n",
      "326         236  Join us at the 2020 Gulf of Mexico Oil Spill &...   \n",
      "327         237  RT @NASEM_Gulf: Join us at the 2020 Gulf of Me...   \n",
      "328         238  ‚Å¶@GulfConference‚Å© Drs Rita Colwell and Ellen W...   \n",
      "329         239  Join us at the 2020 Gulf of Mexico Oil Spill &...   \n",
      "330         240  RT @NASEM_Gulf: Join us at the 2020 Gulf of Me...   \n",
      "331         241  Join us at the 2020 Gulf of Mexico Oil Spill &...   \n",
      "332         242  RT @NASEM_Gulf: Join us at the 2020 Gulf of Me...   \n",
      "333         243  RT @NASEM_Gulf: Join us at the 2020 Gulf of Me...   \n",
      "334         244  Did you catch the world-premiere of the Gulf o...   \n",
      "336         246  RT @ESMaungDouglass: Did you catch the world-p...   \n",
      "377         287  The Cuyahoga river in Cleveland burst into fla...   \n",
      "392         302  @Doldua17 @GovRonDeSantis Word is this could c...   \n",
      "477         387  #OTD in 1993, three vessels collided near the ...   \n",
      "485         395  via ‚Å¶‚Å¶@nytimes‚Å© A manageable risk? This, along...   \n",
      "\n",
      "     possibly_sensitive                   id            author_id  \\\n",
      "6                 False   949380859404038144           1551101437   \n",
      "32                False  1113871426057113600           3438303297   \n",
      "36                False  1113905363726798849           1732895058   \n",
      "37                False  1114036577402064896   971780036368961536   \n",
      "69                False  1550207558534606848            159496534   \n",
      "113               False  1033830354157821952            743758458   \n",
      "139               False  1091086111324192769   863052926469722112   \n",
      "141               False  1092451055122739200           3030402067   \n",
      "143               False  1092507712838135810   742780043416326145   \n",
      "315               False  1169241458525835271  1119240137374871553   \n",
      "316               False  1193223987154882560            937673342   \n",
      "319               False  1212768037855809536           3438303297   \n",
      "322               False  1213185439517024257  1131689631173554177   \n",
      "323               False  1217140627303026688           3438303297   \n",
      "324               False  1217280729039745026            390839075   \n",
      "326               False  1222188099960299520           3438303297   \n",
      "327               False  1222335774500294662           3127920234   \n",
      "328               False  1224345361025785856           1657845170   \n",
      "329               False  1224347414288588802           3438303297   \n",
      "330               False  1224527313032839168            390839075   \n",
      "331               False  1224815128262139908           3438303297   \n",
      "332               False  1224838118815170560            390839075   \n",
      "333               False  1225106825663651840            403517702   \n",
      "334               False  1227328142945193984           3097648455   \n",
      "336               False  1227341517099917312             46137812   \n",
      "377               False  1329082038394130441  1012036197559029763   \n",
      "392               False  1378518953081901065           2551758512   \n",
      "477               False  1425142348024061955           2196784676   \n",
      "485               False  1445050697477607424             39362298   \n",
      "\n",
      "         conversation_id  in_reply_to_user_id reply_settings lang  \\\n",
      "6     949089545953296384         4.907248e+08       everyone   en   \n",
      "32   1113871426057113600                  NaN       everyone   en   \n",
      "36   1113905363726798849                  NaN       everyone   en   \n",
      "37   1114036577402064896                  NaN       everyone   en   \n",
      "69   1550207558534606848                  NaN       everyone   en   \n",
      "113  1033830354157821952                  NaN       everyone   en   \n",
      "139  1091086111324192769                  NaN       everyone   en   \n",
      "141  1092451055122739200                  NaN       everyone   en   \n",
      "143  1092507712838135810                  NaN       everyone   en   \n",
      "315  1169241458525835271                  NaN       everyone   en   \n",
      "316  1193223987154882560                  NaN       everyone   en   \n",
      "319  1212768037855809536                  NaN       everyone   en   \n",
      "322  1213185439517024257                  NaN       everyone   en   \n",
      "323  1217140627303026688                  NaN       everyone   en   \n",
      "324  1217280729039745026                  NaN       everyone   en   \n",
      "326  1222188099960299520                  NaN       everyone   en   \n",
      "327  1222335774500294662                  NaN       everyone   en   \n",
      "328  1224345361025785856                  NaN       everyone   en   \n",
      "329  1224347414288588802                  NaN       everyone   en   \n",
      "330  1224527313032839168                  NaN       everyone   en   \n",
      "331  1224815128262139908                  NaN       everyone   en   \n",
      "332  1224838118815170560                  NaN       everyone   en   \n",
      "333  1225106825663651840                  NaN       everyone   en   \n",
      "334  1227328142945193984                  NaN       everyone   en   \n",
      "336  1227341517099917312                  NaN       everyone   en   \n",
      "377  1329082036527661059         1.012036e+18       everyone   en   \n",
      "392  1378475858332123138         1.246969e+18       everyone   en   \n",
      "477  1425142348024061955                  NaN       everyone   en   \n",
      "485  1445050697477607424                  NaN       everyone   en   \n",
      "\n",
      "            created_at.x  ...  \\\n",
      "6    2018-01-05 20:43:30  ...   \n",
      "32   2019-04-04 18:30:17  ...   \n",
      "36   2019-04-04 20:45:08  ...   \n",
      "37   2019-04-05 05:26:32  ...   \n",
      "69   2022-07-21 19:54:18  ...   \n",
      "113  2018-08-26 21:35:38  ...   \n",
      "139  2019-01-31 21:29:34  ...   \n",
      "141  2019-02-04 15:53:22  ...   \n",
      "143  2019-02-04 19:38:31  ...   \n",
      "315  2019-09-04 13:31:01  ...   \n",
      "316  2019-11-09 17:49:02  ...   \n",
      "319  2020-01-02 16:10:07  ...   \n",
      "322  2020-01-03 19:48:43  ...   \n",
      "323  2020-01-14 17:45:13  ...   \n",
      "324  2020-01-15 03:01:56  ...   \n",
      "326  2020-01-28 16:02:04  ...   \n",
      "327  2020-01-29 01:48:53  ...   \n",
      "328  2020-02-03 14:54:15  ...   \n",
      "329  2020-02-03 15:02:25  ...   \n",
      "330  2020-02-04 02:57:16  ...   \n",
      "331  2020-02-04 22:00:57  ...   \n",
      "332  2020-02-04 23:32:18  ...   \n",
      "333  2020-02-05 17:20:03  ...   \n",
      "334  2020-02-11 20:26:46  ...   \n",
      "336  2020-02-11 21:19:55  ...   \n",
      "377  2020-11-18 15:20:25  ...   \n",
      "392  2021-04-04 01:25:04  ...   \n",
      "477  2021-08-10 17:09:48  ...   \n",
      "485  2021-10-04 15:38:28  ...   \n",
      "\n",
      "                                         description_y  \\\n",
      "6    Surfer, Sailor, Cetacean Researcher & Marine M...   \n",
      "32   The Gulf Research Program of @theNASEM funds s...   \n",
      "36   We are a team of scientists studying the effec...   \n",
      "37   Marine Science Research Institute | Jacksonvil...   \n",
      "69   With 23 academic departments & 14 research cen...   \n",
      "113  M.S. Marine Biologist. Educator Mermaid. Surfe...   \n",
      "139  Discovery begins here. Become a scientific lea...   \n",
      "141  UTMSI is a department of the University of Tex...   \n",
      "143  University of North Texas. Interested in behav...   \n",
      "315  We are the Seascape Lab at Nova Southeastern U...   \n",
      "316  An international consortium studying the impac...   \n",
      "319  The Gulf Research Program of @theNASEM funds s...   \n",
      "322  Are you a UTRGV student with an interest in th...   \n",
      "323  The Gulf Research Program of @theNASEM funds s...   \n",
      "324  professional marine biologist, amateur geologi...   \n",
      "326  The Gulf Research Program of @theNASEM funds s...   \n",
      "327  Hi! my name is Squirt & I will help guide you ...   \n",
      "328  Snr Social and Behavioral Scientist @RANDCorpo...   \n",
      "329  The Gulf Research Program of @theNASEM funds s...   \n",
      "330  professional marine biologist, amateur geologi...   \n",
      "331  The Gulf Research Program of @theNASEM funds s...   \n",
      "332  professional marine biologist, amateur geologi...   \n",
      "333  NGI develops, operates and maintains an integr...   \n",
      "334  Public Engagement Specialist @LASeaGrant. Shar...   \n",
      "336  Louisiana Sea Grant College Program: promoting...   \n",
      "377  The Office of Campus Safety fosters a secure e...   \n",
      "392  Creator of #SidewalkScienceCenter providing sc...   \n",
      "477  @NOAA's Office of Response and Restoration dev...   \n",
      "485  Diver, Cyclist, Hiker, Meteorologist, Speaker ...   \n",
      "\n",
      "                                description_lemmatized  hand.label_simplified  \\\n",
      "6    ['surfer', ',', 'sailor', ',', 'cetacean', 're...                   acad   \n",
      "32   ['the', 'gulf', 'research', 'program', 'of', '...                   acad   \n",
      "36   ['we', 'be', 'a', 'team', 'of', 'scientist', '...                   acad   \n",
      "37   ['marine', 'science', 'research', 'institute',...                   acad   \n",
      "69   ['with', '', 'academic', 'department', '&', ''...                   acad   \n",
      "113  ['m.s', '.', 'marine', 'biologist', '.', 'educ...                   acad   \n",
      "139  ['discovery', 'begin', 'here', '.', 'become', ...                   acad   \n",
      "141  ['utmsi', 'be', 'a', 'department', 'of', 'the'...                   acad   \n",
      "143  ['university', 'of', 'north', 'texas', '.', 'i...                   acad   \n",
      "315  ['we', 'be', 'the', 'seascape', 'lab', 'at', '...                   acad   \n",
      "316  ['an', 'international', 'consortium', 'study',...                   acad   \n",
      "319  ['the', 'gulf', 'research', 'program', 'of', '...                   acad   \n",
      "322  ['be', 'you', 'a', 'utrgv', 'student', 'with',...                   acad   \n",
      "323  ['the', 'gulf', 'research', 'program', 'of', '...                   acad   \n",
      "324  ['professional', 'marine', 'biologist', ',', '...                   acad   \n",
      "326  ['the', 'gulf', 'research', 'program', 'of', '...                   acad   \n",
      "327  ['hi', '!', 'my', 'name', 'be', 'squirt', '&',...                   acad   \n",
      "328  ['snr', 'social', 'and', 'behavioral', 'scient...                   acad   \n",
      "329  ['the', 'gulf', 'research', 'program', 'of', '...                   acad   \n",
      "330  ['professional', 'marine', 'biologist', ',', '...                   acad   \n",
      "331  ['the', 'gulf', 'research', 'program', 'of', '...                   acad   \n",
      "332  ['professional', 'marine', 'biologist', ',', '...                   acad   \n",
      "333  ['ngi', 'develops', ',', 'operates', 'and', 'm...                   acad   \n",
      "334  ['public', 'engagement', 'specialist', '@', 'l...                   acad   \n",
      "336  ['louisiana', 'sea', 'grant', 'college', 'prog...                   acad   \n",
      "377  ['the', 'office', 'of', 'campus', 'safety', 'f...                   acad   \n",
      "392  ['creator', 'of', '#', 'sidewalksciencecenter'...                   acad   \n",
      "477  ['@', 'noaa', \"'s\", 'office', 'of', 'response'...                   acad   \n",
      "485  ['diver', ',', 'cyclist', ',', 'hiker', ',', '...                   acad   \n",
      "\n",
      "    acad_prob  gov_prob  media_prob other_prob tourbiz_prob Label  Label.Type  \n",
      "6    0.830953  0.002286    0.005970   0.144719     0.016073   NaN         NaN  \n",
      "32   0.943721  0.004425    0.001227   0.048869     0.001758   NaN         NaN  \n",
      "36   0.973504  0.000624    0.014026   0.011510     0.000335   NaN         NaN  \n",
      "37   0.998633  0.000393    0.000059   0.000613     0.000302   NaN         NaN  \n",
      "69   0.428769  0.011160    0.006297   0.550501     0.003273   NaN         NaN  \n",
      "113  0.565843  0.000736    0.002633   0.413360     0.017428   NaN         NaN  \n",
      "139  0.940705  0.001007    0.003235   0.054601     0.000452   NaN         NaN  \n",
      "141  0.455684  0.009440    0.002407   0.501951     0.030518   NaN         NaN  \n",
      "143  0.889284  0.000527    0.000374   0.109605     0.000211   NaN         NaN  \n",
      "315  0.979548  0.000130    0.000608   0.018919     0.000796   NaN         NaN  \n",
      "316  0.502699  0.011170    0.045443   0.430871     0.009817   NaN         NaN  \n",
      "319  0.943721  0.004425    0.001227   0.048869     0.001758   NaN         NaN  \n",
      "322  0.974479  0.000302    0.000297   0.023609     0.001312   NaN         NaN  \n",
      "323  0.943721  0.004425    0.001227   0.048869     0.001758   NaN         NaN  \n",
      "324  0.890872  0.000426    0.002851   0.093683     0.012167   NaN         NaN  \n",
      "326  0.943721  0.004425    0.001227   0.048869     0.001758   NaN         NaN  \n",
      "327  0.861513  0.000877    0.019131   0.115883     0.002597   NaN         NaN  \n",
      "328  0.358121  0.001204    0.004025   0.636083     0.000567   NaN         NaN  \n",
      "329  0.943721  0.004425    0.001227   0.048869     0.001758   NaN         NaN  \n",
      "330  0.890872  0.000426    0.002851   0.093683     0.012167   NaN         NaN  \n",
      "331  0.943721  0.004425    0.001227   0.048869     0.001758   NaN         NaN  \n",
      "332  0.890872  0.000426    0.002851   0.093683     0.012167   NaN         NaN  \n",
      "333  0.907320  0.015206    0.003474   0.065890     0.008110   NaN         NaN  \n",
      "334  0.675860  0.006505    0.139571   0.177637     0.000427   NaN         NaN  \n",
      "336  0.869759  0.010423    0.002115   0.108922     0.008781   NaN         NaN  \n",
      "377  0.586994  0.024517    0.076840   0.307926     0.003724   NaN         NaN  \n",
      "392  0.373893  0.029652    0.199901   0.377250     0.019305   NaN         NaN  \n",
      "477  0.552314  0.046727    0.037037   0.357191     0.006732   NaN         NaN  \n",
      "485  0.392464  0.007208    0.295117   0.290578     0.014632   NaN         NaN  \n",
      "\n",
      "[29 rows x 42 columns]\n"
     ]
    }
   ],
   "source": [
    "# preprocessing for tf-idf: columns -> account type, word, n (number of accounts that mentioned that word)\n",
    "oil_acad = oil_w_acc[oil_w_acc[\"hand.label_simplified\"]==\"acad\"]\n",
    "print(oil_acad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "41768604-b237-49e9-b023-329493cec7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = ['Gottfried Creek', 'Halifax River', 'Hamilton Branch', 'Harney River', 'Hickory Creek','Hillsborough River', 'Homosassa River', 'Hontoon Dead River', 'Ichetucknee River', 'Imperial River',\n",
    "        'Itchepackesassa Creek', 'Julington Creek', 'Kissimmee River', 'Lafayette Creek',\"Little Econlockhatchee River\", \"Little Manatee River\", \"Little River (Biscayne Bay)\",\n",
    "        \"Little River (Ochlockonee River tributary)\", \"Little Wekiva River\", \"Little Withlacoochee River\",\"Lochloosa Creek\", \"Loxahatchee River\", \"Manatee River\", \"Matanzas River\", \"McCullough Creek\",\n",
    "        \"Miami River\", \"Myakka River\", \"Myakkahatchee Creek\", \"New River (Broward County)\",\"New River (Carrabelle River tributary)\", \"New River (Santa Fe River tributary)\", \"Ochlockonee River\",\n",
    "        \"Ocklawaha River\", \"Oleta River\", \"Orange Creek\", \"Orange River\", \"Palatlakaha River\", \"Payne Creek\",\"Pea River\", \"Peace River\", \"Pellicer Creek\", \"Perdido River\", \"Pimple Creek\",\n",
    "        \"Pithlachascotee River\", \"Poplar Creek\", \"Pottsburg Creek\", \"Rainbow River\", \"Ribault River\",\"Rice Creek (St. Johns River)\", \"Rocky Comfort Creek\", \"St. Johns River\", \"St. Marys River\",\n",
    "        \"Santa Fe River\", \"Shingle Creek\", \"Silver River\", \"Snapper Creek\", \"Sopchoppy River\",\"Spanish River\", \"Spanishtown Creek\", \"St. Lucie River\", \"St. Marks River\", \"St. Sebastian River\",\n",
    "        \"Steinhatchee River\", \"Suwannee River\", \"Swamp Creek (Attapulgus Creek tributary)\",\"Tampa Bypass Canal\", \"Telogia Creek\", \"Three Rivers State Park\", \"Tiger Creek\", \"Tomoka River\",\n",
    "        \"Trout River\", \"Turkey Creek (Econlockhatchee River tributary)\",\"Turkey Creek (Indian River tributary)\", \"Waccasassa River\", \"Wacissa River\", \"Wagner Creek\",\n",
    "        \"Wakulla River\", \"Weeki Wachee River\", \"Wekiva River\", \"Wekiva River (Gulf Hammock, Levy County)\",\"Whidden Creek\", \"Withlacoochee River\", \"Withlacoochee River (Suwannee River tributary)\",\n",
    "        \"Yellow River (Pensacola Bay)\", 'Apalachee Bay', 'Apalachicola Bay', 'Boca Ciega Bay', 'Charlotte Harbor','Choctawhatchee Bay', 'East Bay', 'Escambia Bay', 'Estero Bay', 'Florida Bay', 'Pensacola Bay', 'Ponce de Leon Bay',\n",
    "        'Sarasota Bay', 'St. Andrews Bay', 'St. Joseph Bay', 'Tampa Bay', 'Whitewater Bay', \"Anna Maria\", \"Bradenton\",\"Bradenton Beach\", \"Cortez\", \"Ellenton\", \"Holmes Beach\", \"Longboat Key\",\n",
    "        \"Myakka City\", \"Oneco\", \"Palmetto\", \"Parrish\", \"Sarasota\", \"Tallevast\", \"Terra Ceia\", \"Apollo Beach\",\"Balm\", \"Brandon\", \"Dover\", \"Durant\", \"Gibsonton\", \"Lithia\", \"Lutz\", \"Mango\", \"Odessa\", \"Plant City\",\n",
    "        \"Riverview\", \"Ruskin\", \"Seffner\", \"Sun City\", \"Sun City Center\", \"Sydney\", \"Tampa\", \"Thonotosassa\",\"Valrico\", \"Wimauma\", \"Bay Pines\", \"Belleair Beach\", \"Clearwater\", \"Clearwater Beach\", \"Crystal Beach\",\n",
    "        \"Dunedin\", \"Indian Rocks Beach\", \"Largo\", \"Oldsmar\", \"Ozona\", \"Palm Harbor\", \"Pinellas Park\",\"Safety Harbor\", \"Saint Petersburg\", \"Seminole\", \"Tarpon Springs\", \"Aripeka\", \"Crystal Springs\",\n",
    "        \"Dade City\", \"Holiday\", \"Hudson\", \"Lacoochee\", \"Land O Lakes\", \"New Port Richey\", \"Port Richey\",\"Saint Leo\", \"San Antonio\", \"Spring Hill\", \"Trilby\", \"Wesley Chapel\", \"Zephyrhills\", \"fl\",\n",
    "        \"florida\", \"swfl\", \"floridas\", \"manateecounty\", \"annamariaisland\", \"siestakey\", \"stpete\", \"sanibel\", \"everglades\", \"fortmyersbeach\",\"manasotakey\", \"sarasotabay\", \"fortmyers\", \"lakeokeechobee\", \"bradentonbeach\",\n",
    "        \"formyers\", \"bocagrande\",\"siesta\", \"florda\", \"srq\", \"sarastoabay\", \"stpetersburg\", \"tampabay\", \"pinellascounty\", \"pineypoint\", \"clearwaterbeach\", \"capecoral\", \"gulfofmexico\",\"portcharlotte\",\"bay\", \"gulf\",\"st\",\"pete\", \"St.Petersburg\"\n",
    "        \"petersburg\", \"bay\", \"gulf\", \"mexico\",\n",
    "]\n",
    "locations = [x.lower() for x in locations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4a2ff38f-f817-4e8f-823c-f552fecb7569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Unnamed: 0                                               text  \\\n",
      "6             1  @jamesapisano The Barataria Bay Bottlenose dol...   \n",
      "32            9  Join researchers and responders as they discus...   \n",
      "36           13  RT @NASEM_Gulf: Join researchers and responder...   \n",
      "37           14  RT @NASEM_Gulf: Join researchers and responder...   \n",
      "69           46  RT @usfsp: Associate Professor Heather Judkins...   \n",
      "113          23  As a scientist..we're so frustrated. Politics ...   \n",
      "139          49  RT @RECOVER_GOMRI: #tbt to 2016 Gulf of Mexico...   \n",
      "141          51  RT @RECOVER_GOMRI: #tbt to 2016 Gulf of Mexico...   \n",
      "143          53  RT @RECOVER_GOMRI: #tbt to 2016 Gulf of Mexico...   \n",
      "315         225  We are very pleased to be running a pelagic ec...   \n",
      "316         226  Steve Murawski presented at Tampa Bay's Festiv...   \n",
      "319         229  Join us at the 2020 Gulf of Mexico Oil Spill &...   \n",
      "322         232  RT @NASEM_Gulf: Join us at the 2020 Gulf of Me...   \n",
      "323         233  Join us at the 2020 Gulf of Mexico Oil Spill &...   \n",
      "324         234  RT @NASEM_Gulf: Join us at the 2020 Gulf of Me...   \n",
      "326         236  Join us at the 2020 Gulf of Mexico Oil Spill &...   \n",
      "327         237  RT @NASEM_Gulf: Join us at the 2020 Gulf of Me...   \n",
      "328         238  ‚Å¶@GulfConference‚Å© Drs Rita Colwell and Ellen W...   \n",
      "329         239  Join us at the 2020 Gulf of Mexico Oil Spill &...   \n",
      "330         240  RT @NASEM_Gulf: Join us at the 2020 Gulf of Me...   \n",
      "331         241  Join us at the 2020 Gulf of Mexico Oil Spill &...   \n",
      "332         242  RT @NASEM_Gulf: Join us at the 2020 Gulf of Me...   \n",
      "333         243  RT @NASEM_Gulf: Join us at the 2020 Gulf of Me...   \n",
      "334         244  Did you catch the world-premiere of the Gulf o...   \n",
      "336         246  RT @ESMaungDouglass: Did you catch the world-p...   \n",
      "377         287  The Cuyahoga river in Cleveland burst into fla...   \n",
      "392         302  @Doldua17 @GovRonDeSantis Word is this could c...   \n",
      "477         387  #OTD in 1993, three vessels collided near the ...   \n",
      "485         395  via ‚Å¶‚Å¶@nytimes‚Å© A manageable risk? This, along...   \n",
      "\n",
      "     possibly_sensitive                   id            author_id  \\\n",
      "6                 False   949380859404038144           1551101437   \n",
      "32                False  1113871426057113600           3438303297   \n",
      "36                False  1113905363726798849           1732895058   \n",
      "37                False  1114036577402064896   971780036368961536   \n",
      "69                False  1550207558534606848            159496534   \n",
      "113               False  1033830354157821952            743758458   \n",
      "139               False  1091086111324192769   863052926469722112   \n",
      "141               False  1092451055122739200           3030402067   \n",
      "143               False  1092507712838135810   742780043416326145   \n",
      "315               False  1169241458525835271  1119240137374871553   \n",
      "316               False  1193223987154882560            937673342   \n",
      "319               False  1212768037855809536           3438303297   \n",
      "322               False  1213185439517024257  1131689631173554177   \n",
      "323               False  1217140627303026688           3438303297   \n",
      "324               False  1217280729039745026            390839075   \n",
      "326               False  1222188099960299520           3438303297   \n",
      "327               False  1222335774500294662           3127920234   \n",
      "328               False  1224345361025785856           1657845170   \n",
      "329               False  1224347414288588802           3438303297   \n",
      "330               False  1224527313032839168            390839075   \n",
      "331               False  1224815128262139908           3438303297   \n",
      "332               False  1224838118815170560            390839075   \n",
      "333               False  1225106825663651840            403517702   \n",
      "334               False  1227328142945193984           3097648455   \n",
      "336               False  1227341517099917312             46137812   \n",
      "377               False  1329082038394130441  1012036197559029763   \n",
      "392               False  1378518953081901065           2551758512   \n",
      "477               False  1425142348024061955           2196784676   \n",
      "485               False  1445050697477607424             39362298   \n",
      "\n",
      "         conversation_id  in_reply_to_user_id reply_settings lang  \\\n",
      "6     949089545953296384         4.907248e+08       everyone   en   \n",
      "32   1113871426057113600                  NaN       everyone   en   \n",
      "36   1113905363726798849                  NaN       everyone   en   \n",
      "37   1114036577402064896                  NaN       everyone   en   \n",
      "69   1550207558534606848                  NaN       everyone   en   \n",
      "113  1033830354157821952                  NaN       everyone   en   \n",
      "139  1091086111324192769                  NaN       everyone   en   \n",
      "141  1092451055122739200                  NaN       everyone   en   \n",
      "143  1092507712838135810                  NaN       everyone   en   \n",
      "315  1169241458525835271                  NaN       everyone   en   \n",
      "316  1193223987154882560                  NaN       everyone   en   \n",
      "319  1212768037855809536                  NaN       everyone   en   \n",
      "322  1213185439517024257                  NaN       everyone   en   \n",
      "323  1217140627303026688                  NaN       everyone   en   \n",
      "324  1217280729039745026                  NaN       everyone   en   \n",
      "326  1222188099960299520                  NaN       everyone   en   \n",
      "327  1222335774500294662                  NaN       everyone   en   \n",
      "328  1224345361025785856                  NaN       everyone   en   \n",
      "329  1224347414288588802                  NaN       everyone   en   \n",
      "330  1224527313032839168                  NaN       everyone   en   \n",
      "331  1224815128262139908                  NaN       everyone   en   \n",
      "332  1224838118815170560                  NaN       everyone   en   \n",
      "333  1225106825663651840                  NaN       everyone   en   \n",
      "334  1227328142945193984                  NaN       everyone   en   \n",
      "336  1227341517099917312                  NaN       everyone   en   \n",
      "377  1329082036527661059         1.012036e+18       everyone   en   \n",
      "392  1378475858332123138         1.246969e+18       everyone   en   \n",
      "477  1425142348024061955                  NaN       everyone   en   \n",
      "485  1445050697477607424                  NaN       everyone   en   \n",
      "\n",
      "            created_at.x  ...  \\\n",
      "6    2018-01-05 20:43:30  ...   \n",
      "32   2019-04-04 18:30:17  ...   \n",
      "36   2019-04-04 20:45:08  ...   \n",
      "37   2019-04-05 05:26:32  ...   \n",
      "69   2022-07-21 19:54:18  ...   \n",
      "113  2018-08-26 21:35:38  ...   \n",
      "139  2019-01-31 21:29:34  ...   \n",
      "141  2019-02-04 15:53:22  ...   \n",
      "143  2019-02-04 19:38:31  ...   \n",
      "315  2019-09-04 13:31:01  ...   \n",
      "316  2019-11-09 17:49:02  ...   \n",
      "319  2020-01-02 16:10:07  ...   \n",
      "322  2020-01-03 19:48:43  ...   \n",
      "323  2020-01-14 17:45:13  ...   \n",
      "324  2020-01-15 03:01:56  ...   \n",
      "326  2020-01-28 16:02:04  ...   \n",
      "327  2020-01-29 01:48:53  ...   \n",
      "328  2020-02-03 14:54:15  ...   \n",
      "329  2020-02-03 15:02:25  ...   \n",
      "330  2020-02-04 02:57:16  ...   \n",
      "331  2020-02-04 22:00:57  ...   \n",
      "332  2020-02-04 23:32:18  ...   \n",
      "333  2020-02-05 17:20:03  ...   \n",
      "334  2020-02-11 20:26:46  ...   \n",
      "336  2020-02-11 21:19:55  ...   \n",
      "377  2020-11-18 15:20:25  ...   \n",
      "392  2021-04-04 01:25:04  ...   \n",
      "477  2021-08-10 17:09:48  ...   \n",
      "485  2021-10-04 15:38:28  ...   \n",
      "\n",
      "                                description_lemmatized hand.label_simplified  \\\n",
      "6    ['surfer', ',', 'sailor', ',', 'cetacean', 're...                  acad   \n",
      "32   ['the', 'gulf', 'research', 'program', 'of', '...                  acad   \n",
      "36   ['we', 'be', 'a', 'team', 'of', 'scientist', '...                  acad   \n",
      "37   ['marine', 'science', 'research', 'institute',...                  acad   \n",
      "69   ['with', '', 'academic', 'department', '&', ''...                  acad   \n",
      "113  ['m.s', '.', 'marine', 'biologist', '.', 'educ...                  acad   \n",
      "139  ['discovery', 'begin', 'here', '.', 'become', ...                  acad   \n",
      "141  ['utmsi', 'be', 'a', 'department', 'of', 'the'...                  acad   \n",
      "143  ['university', 'of', 'north', 'texas', '.', 'i...                  acad   \n",
      "315  ['we', 'be', 'the', 'seascape', 'lab', 'at', '...                  acad   \n",
      "316  ['an', 'international', 'consortium', 'study',...                  acad   \n",
      "319  ['the', 'gulf', 'research', 'program', 'of', '...                  acad   \n",
      "322  ['be', 'you', 'a', 'utrgv', 'student', 'with',...                  acad   \n",
      "323  ['the', 'gulf', 'research', 'program', 'of', '...                  acad   \n",
      "324  ['professional', 'marine', 'biologist', ',', '...                  acad   \n",
      "326  ['the', 'gulf', 'research', 'program', 'of', '...                  acad   \n",
      "327  ['hi', '!', 'my', 'name', 'be', 'squirt', '&',...                  acad   \n",
      "328  ['snr', 'social', 'and', 'behavioral', 'scient...                  acad   \n",
      "329  ['the', 'gulf', 'research', 'program', 'of', '...                  acad   \n",
      "330  ['professional', 'marine', 'biologist', ',', '...                  acad   \n",
      "331  ['the', 'gulf', 'research', 'program', 'of', '...                  acad   \n",
      "332  ['professional', 'marine', 'biologist', ',', '...                  acad   \n",
      "333  ['ngi', 'develops', ',', 'operates', 'and', 'm...                  acad   \n",
      "334  ['public', 'engagement', 'specialist', '@', 'l...                  acad   \n",
      "336  ['louisiana', 'sea', 'grant', 'college', 'prog...                  acad   \n",
      "377  ['the', 'office', 'of', 'campus', 'safety', 'f...                  acad   \n",
      "392  ['creator', 'of', '#', 'sidewalksciencecenter'...                  acad   \n",
      "477  ['@', 'noaa', \"'s\", 'office', 'of', 'response'...                  acad   \n",
      "485  ['diver', ',', 'cyclist', ',', 'hiker', ',', '...                  acad   \n",
      "\n",
      "     acad_prob  gov_prob media_prob  other_prob tourbiz_prob Label Label.Type  \\\n",
      "6     0.830953  0.002286   0.005970    0.144719     0.016073   NaN        NaN   \n",
      "32    0.943721  0.004425   0.001227    0.048869     0.001758   NaN        NaN   \n",
      "36    0.973504  0.000624   0.014026    0.011510     0.000335   NaN        NaN   \n",
      "37    0.998633  0.000393   0.000059    0.000613     0.000302   NaN        NaN   \n",
      "69    0.428769  0.011160   0.006297    0.550501     0.003273   NaN        NaN   \n",
      "113   0.565843  0.000736   0.002633    0.413360     0.017428   NaN        NaN   \n",
      "139   0.940705  0.001007   0.003235    0.054601     0.000452   NaN        NaN   \n",
      "141   0.455684  0.009440   0.002407    0.501951     0.030518   NaN        NaN   \n",
      "143   0.889284  0.000527   0.000374    0.109605     0.000211   NaN        NaN   \n",
      "315   0.979548  0.000130   0.000608    0.018919     0.000796   NaN        NaN   \n",
      "316   0.502699  0.011170   0.045443    0.430871     0.009817   NaN        NaN   \n",
      "319   0.943721  0.004425   0.001227    0.048869     0.001758   NaN        NaN   \n",
      "322   0.974479  0.000302   0.000297    0.023609     0.001312   NaN        NaN   \n",
      "323   0.943721  0.004425   0.001227    0.048869     0.001758   NaN        NaN   \n",
      "324   0.890872  0.000426   0.002851    0.093683     0.012167   NaN        NaN   \n",
      "326   0.943721  0.004425   0.001227    0.048869     0.001758   NaN        NaN   \n",
      "327   0.861513  0.000877   0.019131    0.115883     0.002597   NaN        NaN   \n",
      "328   0.358121  0.001204   0.004025    0.636083     0.000567   NaN        NaN   \n",
      "329   0.943721  0.004425   0.001227    0.048869     0.001758   NaN        NaN   \n",
      "330   0.890872  0.000426   0.002851    0.093683     0.012167   NaN        NaN   \n",
      "331   0.943721  0.004425   0.001227    0.048869     0.001758   NaN        NaN   \n",
      "332   0.890872  0.000426   0.002851    0.093683     0.012167   NaN        NaN   \n",
      "333   0.907320  0.015206   0.003474    0.065890     0.008110   NaN        NaN   \n",
      "334   0.675860  0.006505   0.139571    0.177637     0.000427   NaN        NaN   \n",
      "336   0.869759  0.010423   0.002115    0.108922     0.008781   NaN        NaN   \n",
      "377   0.586994  0.024517   0.076840    0.307926     0.003724   NaN        NaN   \n",
      "392   0.373893  0.029652   0.199901    0.377250     0.019305   NaN        NaN   \n",
      "477   0.552314  0.046727   0.037037    0.357191     0.006732   NaN        NaN   \n",
      "485   0.392464  0.007208   0.295117    0.290578     0.014632   NaN        NaN   \n",
      "\n",
      "                                                 words  \n",
      "6    [barataria, bay, bottlenose, dolphin, populati...  \n",
      "32   [join, researcher, responder, discuss, science...  \n",
      "36   [join, researcher, responder, discuss, science...  \n",
      "37   [join, researcher, responder, discuss, science...  \n",
      "69   [associate, professor, heather, judkins, disco...  \n",
      "113  [scientist, frustrate, politics, fund, prevent...  \n",
      "139  [tbt, gulf, mexico, oil, spill, ecosystem, sci...  \n",
      "141  [tbt, gulf, mexico, oil, spill, ecosystem, sci...  \n",
      "143  [tbt, gulf, mexico, oil, spill, ecosystem, sci...  \n",
      "315  [pleased, run, pelagic, ecology, session, gulf...  \n",
      "316  [steve, murawski, present, tampa, festival, re...  \n",
      "319  [join, gulf, mexico, oil, spill, amp, ecosyste...  \n",
      "322  [join, gulf, mexico, oil, spill, amp, ecosyste...  \n",
      "323  [join, gulf, mexico, oil, spill, amp, ecosyste...  \n",
      "324  [join, gulf, mexico, oil, spill, amp, ecosyste...  \n",
      "326  [join, gulf, mexico, oil, spill, amp, ecosyste...  \n",
      "327  [join, gulf, mexico, oil, spill, amp, ecosyste...  \n",
      "328  [drs, rita, colwell, ellen, williams, reflect,...  \n",
      "329  [join, gulf, mexico, oil, spill, amp, ecosyste...  \n",
      "330  [join, gulf, mexico, oil, spill, amp, ecosyste...  \n",
      "331  [join, gulf, mexico, oil, spill, amp, ecosyste...  \n",
      "332  [join, gulf, mexico, oil, spill, amp, ecosyste...  \n",
      "333  [join, gulf, mexico, oil, spill, amp, ecosyste...  \n",
      "334  [catch, premiere, gulf, mexico, oil, spill, sc...  \n",
      "336  [catch, premiere, gulf, mexico, oil, spill, sc...  \n",
      "377  [cuyahoga, river, cleveland, burst, flame, sep...  \n",
      "392  [word, ecological, disaster, bad, oil, spill, ...  \n",
      "477  [otd, vessel, collide, entrance, tampa, bay, f...  \n",
      "485  [manageable, risk, oil, spill, year, san, fran...  \n",
      "\n",
      "[29 rows x 43 columns]\n",
      "     Unnamed: 0                                               text  \\\n",
      "6             1  @jamesapisano The Barataria Bay Bottlenose dol...   \n",
      "6             1  @jamesapisano The Barataria Bay Bottlenose dol...   \n",
      "6             1  @jamesapisano The Barataria Bay Bottlenose dol...   \n",
      "6             1  @jamesapisano The Barataria Bay Bottlenose dol...   \n",
      "6             1  @jamesapisano The Barataria Bay Bottlenose dol...   \n",
      "..          ...                                                ...   \n",
      "485         395  via ‚Å¶‚Å¶@nytimes‚Å© A manageable risk? This, along...   \n",
      "485         395  via ‚Å¶‚Å¶@nytimes‚Å© A manageable risk? This, along...   \n",
      "485         395  via ‚Å¶‚Å¶@nytimes‚Å© A manageable risk? This, along...   \n",
      "485         395  via ‚Å¶‚Å¶@nytimes‚Å© A manageable risk? This, along...   \n",
      "485         395  via ‚Å¶‚Å¶@nytimes‚Å© A manageable risk? This, along...   \n",
      "\n",
      "     possibly_sensitive                   id   author_id      conversation_id  \\\n",
      "6                 False   949380859404038144  1551101437   949089545953296384   \n",
      "6                 False   949380859404038144  1551101437   949089545953296384   \n",
      "6                 False   949380859404038144  1551101437   949089545953296384   \n",
      "6                 False   949380859404038144  1551101437   949089545953296384   \n",
      "6                 False   949380859404038144  1551101437   949089545953296384   \n",
      "..                  ...                  ...         ...                  ...   \n",
      "485               False  1445050697477607424    39362298  1445050697477607424   \n",
      "485               False  1445050697477607424    39362298  1445050697477607424   \n",
      "485               False  1445050697477607424    39362298  1445050697477607424   \n",
      "485               False  1445050697477607424    39362298  1445050697477607424   \n",
      "485               False  1445050697477607424    39362298  1445050697477607424   \n",
      "\n",
      "     in_reply_to_user_id reply_settings lang         created_at.x  ...  \\\n",
      "6            490724761.0       everyone   en  2018-01-05 20:43:30  ...   \n",
      "6            490724761.0       everyone   en  2018-01-05 20:43:30  ...   \n",
      "6            490724761.0       everyone   en  2018-01-05 20:43:30  ...   \n",
      "6            490724761.0       everyone   en  2018-01-05 20:43:30  ...   \n",
      "6            490724761.0       everyone   en  2018-01-05 20:43:30  ...   \n",
      "..                   ...            ...  ...                  ...  ...   \n",
      "485                  NaN       everyone   en  2021-10-04 15:38:28  ...   \n",
      "485                  NaN       everyone   en  2021-10-04 15:38:28  ...   \n",
      "485                  NaN       everyone   en  2021-10-04 15:38:28  ...   \n",
      "485                  NaN       everyone   en  2021-10-04 15:38:28  ...   \n",
      "485                  NaN       everyone   en  2021-10-04 15:38:28  ...   \n",
      "\n",
      "                                description_lemmatized hand.label_simplified  \\\n",
      "6    ['surfer', ',', 'sailor', ',', 'cetacean', 're...                  acad   \n",
      "6    ['surfer', ',', 'sailor', ',', 'cetacean', 're...                  acad   \n",
      "6    ['surfer', ',', 'sailor', ',', 'cetacean', 're...                  acad   \n",
      "6    ['surfer', ',', 'sailor', ',', 'cetacean', 're...                  acad   \n",
      "6    ['surfer', ',', 'sailor', ',', 'cetacean', 're...                  acad   \n",
      "..                                                 ...                   ...   \n",
      "485  ['diver', ',', 'cyclist', ',', 'hiker', ',', '...                  acad   \n",
      "485  ['diver', ',', 'cyclist', ',', 'hiker', ',', '...                  acad   \n",
      "485  ['diver', ',', 'cyclist', ',', 'hiker', ',', '...                  acad   \n",
      "485  ['diver', ',', 'cyclist', ',', 'hiker', ',', '...                  acad   \n",
      "485  ['diver', ',', 'cyclist', ',', 'hiker', ',', '...                  acad   \n",
      "\n",
      "     acad_prob  gov_prob media_prob  other_prob tourbiz_prob Label Label.Type  \\\n",
      "6     0.830953  0.002286   0.005970    0.144719     0.016073   NaN        NaN   \n",
      "6     0.830953  0.002286   0.005970    0.144719     0.016073   NaN        NaN   \n",
      "6     0.830953  0.002286   0.005970    0.144719     0.016073   NaN        NaN   \n",
      "6     0.830953  0.002286   0.005970    0.144719     0.016073   NaN        NaN   \n",
      "6     0.830953  0.002286   0.005970    0.144719     0.016073   NaN        NaN   \n",
      "..         ...       ...        ...         ...          ...   ...        ...   \n",
      "485   0.392464  0.007208   0.295117    0.290578     0.014632   NaN        NaN   \n",
      "485   0.392464  0.007208   0.295117    0.290578     0.014632   NaN        NaN   \n",
      "485   0.392464  0.007208   0.295117    0.290578     0.014632   NaN        NaN   \n",
      "485   0.392464  0.007208   0.295117    0.290578     0.014632   NaN        NaN   \n",
      "485   0.392464  0.007208   0.295117    0.290578     0.014632   NaN        NaN   \n",
      "\n",
      "          words  \n",
      "6     barataria  \n",
      "6           bay  \n",
      "6    bottlenose  \n",
      "6       dolphin  \n",
      "6    population  \n",
      "..          ...  \n",
      "485       tampa  \n",
      "485         bay  \n",
      "485   louisiana  \n",
      "485       ocean  \n",
      "485       spill  \n",
      "\n",
      "[581 rows x 43 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v1/dkrd1tzx2cvd0z9f5pbj8xp00000gn/T/ipykernel_35568/1316496190.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  oil_acad['words'] = oil_acad['text_with_display_links'].str.split()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Splitting words in each text_with_display_links entry\n",
    "oil_acad['words'] = oil_acad['text_with_display_links'].str.split()\n",
    "print(oil_acad)\n",
    "\n",
    "# Exploding to create rows for each word\n",
    "oil_acad = oil_acad.explode('words')\n",
    "\n",
    "print(oil_acad)\n",
    "\n",
    "# Define stop words (words you want to exclude)\n",
    "oil_stop_words = [\"oil\", \"crude\", \"petroleum\", \"tar ball\", \"tar balls\",\"leak\",\"leaks\",\"leaked\", \"leaking\", \"leakage\", \"spill\", \"spills\", \"spilled\", \"spilling\", \"spillage\", \"ocean\", \"beach\", \"beaches\", \"bay\", \"gulf\", \"sea\", \"lake\", \"river\", \"creek\", \"waterway\"]\n",
    "\n",
    "# Filter out the locations and search words\n",
    "oil_acad = oil_acad[~oil_acad['words'].isin(locations)]\n",
    "oil_acad = oil_acad[~oil_acad['words'].isin(oil_stop_words)]\n",
    "\n",
    "# Dropping duplicates of user and words to get unique user-word pairs\n",
    "unique_user_word_pairs = oil_acad[['username', 'words']].drop_duplicates()\n",
    "\n",
    "# Counting unique users per word\n",
    "word_usage_count = unique_user_word_pairs.groupby('words')['username'].nunique().reset_index(name='count')\n",
    "\n",
    "# adding the acc type column\n",
    "word_usage_count['acc_type'] = \"acad\"\n",
    "\n",
    "# Reordering columns\n",
    "word_usage_count = word_usage_count[['acc_type', 'words', 'count']]\n",
    "\n",
    "# Count how many unique words have been used by at least one user\n",
    "unique_word_count = word_usage_count['words'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3141ea97-2da3-4c6e-9450-fe5a9d01f50c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0bb7814a-d0b4-4cfe-b942-4fddc4c850a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc_type</th>\n",
       "      <th>words</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>acad</td>\n",
       "      <td>science</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>acad</td>\n",
       "      <td>conference</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>acad</td>\n",
       "      <td>outreach</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>acad</td>\n",
       "      <td>gomoses</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>acad</td>\n",
       "      <td>ecosystem</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>acad</td>\n",
       "      <td>judkins</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>acad</td>\n",
       "      <td>cruise</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>acad</td>\n",
       "      <td>jet</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>acad</td>\n",
       "      <td>initiative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>acad</td>\n",
       "      <td>pa</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>162 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    acc_type       words  count\n",
       "135     acad     science     14\n",
       "25      acad  conference      9\n",
       "101     acad    outreach      8\n",
       "66      acad     gomoses      8\n",
       "44      acad   ecosystem      8\n",
       "..       ...         ...    ...\n",
       "82      acad     judkins      1\n",
       "28      acad      cruise      1\n",
       "80      acad         jet      1\n",
       "79      acad  initiative      1\n",
       "102     acad          pa      1\n",
       "\n",
       "[162 rows x 3 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_usage_count.sort_values(by='count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "30c1cec4-cc0d-4158-98a1-c07e323f9a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_oil_acad = word_usage_count.sort_values(by='count', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1ed3f3fa-a030-4ba4-a5da-1ee968c21a75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc_type</th>\n",
       "      <th>words</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>acad</td>\n",
       "      <td>science</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>acad</td>\n",
       "      <td>conference</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>acad</td>\n",
       "      <td>outreach</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>acad</td>\n",
       "      <td>gomoses</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>acad</td>\n",
       "      <td>ecosystem</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>acad</td>\n",
       "      <td>learn</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>acad</td>\n",
       "      <td>join</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>acad</td>\n",
       "      <td>week</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>acad</td>\n",
       "      <td>program</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>acad</td>\n",
       "      <td>session</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    acc_type       words  count\n",
       "135     acad     science     14\n",
       "25      acad  conference      9\n",
       "101     acad    outreach      8\n",
       "66      acad     gomoses      8\n",
       "44      acad   ecosystem      8\n",
       "86      acad       learn      7\n",
       "81      acad        join      7\n",
       "157     acad        week      6\n",
       "117     acad     program      6\n",
       "138     acad     session      5"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_10_oil_acad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78be0387-04de-439a-9948-eec7db61d86f",
   "metadata": {},
   "source": [
    "## Let's turn this into a function to apply to all!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b2fa2618-bbe8-455b-ba3a-c7be592c43d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf_preprocessing(file, acc_type, search_stop_words, location_stop_words, print_stat):\n",
    "    file_acc_typed = file[file[\"hand.label_simplified\"]==acc_type]\n",
    "    # Splitting words in each text_with_display_links entry\n",
    "    file_acc_typed['words'] = file_acc_typed['text_with_display_links'].str.split()\n",
    "    \n",
    "    # Exploding to create rows for each word\n",
    "    file_acc_typed = file_acc_typed.explode('words')\n",
    "    \n",
    "    # Filter out the locations and search words\n",
    "    file_acc_typed = file_acc_typed[~file_acc_typed['words'].isin(locations)]\n",
    "    file_acc_typed = file_acc_typed[~file_acc_typed['words'].isin(search_stop_words)]\n",
    "    \n",
    "    # Dropping duplicates of user and words to get unique user-word pairs\n",
    "    unique_user_word_pairs = file_acc_typed[['username', 'words']].drop_duplicates()\n",
    "    \n",
    "    # Counting unique users per word\n",
    "    word_usage_count = unique_user_word_pairs.groupby('words')['username'].nunique().reset_index(name='count')\n",
    "    \n",
    "    # adding the acc type column\n",
    "    word_usage_count['acc_type'] = acc_type\n",
    "    \n",
    "    # Reordering columns\n",
    "    word_usage_count = word_usage_count[['acc_type', 'words', 'count']]\n",
    "    \n",
    "    # Count how many unique words have been used by at least one user\n",
    "    unique_word_count = word_usage_count['words'].nunique()\n",
    "\n",
    "    print(print_stat)\n",
    "    print(word_usage_count.sort_values(by='count', ascending=False).head(10))\n",
    "    word_usage_count.to_csv(f\"not_merged_tf_idf/{print_stat}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3b9f0363-07c0-487d-81f6-215bd504e441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords\n",
    "oil_stop_words = [\"oil\", \"crude\", \"petroleum\", \"tar ball\", \"tar balls\",\"leak\",\"leaks\",\"leaked\", \"leaking\", \"leakage\", \"spill\", \"spills\", \"spilled\", \"spilling\", \"spillage\", \"ocean\", \"beach\", \"beaches\", \"bay\", \"gulf\", \"sea\", \"lake\", \"river\", \"creek\", \"waterway\"]\n",
    "sewage_stop_words=[\"wastewater\", \"sewer\", \"sewers\", \"sewage\", \"septic\", \"stormwater\", \"storm water\", \"reclaimed water\", \"reclaim water\", \"untreated\", \"raw\", \"overflow\", \"discharge\", \"discharges\", \"discharging\", \"discharged\", \"pump\", \"pumps\", \"pumping\", \"pumped\", \"leak\", \"leaks\", \"leaked\", \"leaking\", \"leakage\", \"spill\", \"spills\", \"spilled\", \"spilling\", \"spillage\", \"dump\", \"dumps\", \"dumped\", \"dumping\",\"ocean\", \"beach\", \"beaches\", \"bay\", \"gulf\", \"sea\", \"lake\", \"river\", \"creek\", \"waterway\"]\n",
    "industrial_stop_words = [\"wastewater\",\"contaminants\", \"contamination\", \"contaminating\", \"chemical\", \"chemicals\", \"discharge\", \"discharges\", \"discharging\", \"discharged\", \"pump\", \"pumps\", \"pumping\", \"pumped\", \"leak\", \"leaks\", \"leaked\", \"leaking\", \"leakage\", \"spill\", \"spills\", \"spilled\", \"spilling\", \"spillage\",\"dump\", \"dumps\", \"dumped\",\"dumping\", \"ocean\", \"beach\", \"beaches\", \"bay\", \"gulf\", \"sea\", \"lake\", \"river\", \"creek\", \"waterway\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7781ac91-731b-4eda-88d4-11a6b3715d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oil_acad\n",
      "Empty DataFrame\n",
      "Columns: [acc_type, words, count]\n",
      "Index: []\n",
      "oil_gov\n",
      "Empty DataFrame\n",
      "Columns: [acc_type, words, count]\n",
      "Index: []\n",
      "oil_media\n",
      "Empty DataFrame\n",
      "Columns: [acc_type, words, count]\n",
      "Index: []\n",
      "oil_other\n",
      "Empty DataFrame\n",
      "Columns: [acc_type, words, count]\n",
      "Index: []\n",
      "oil_tourbiz\n",
      "Empty DataFrame\n",
      "Columns: [acc_type, words, count]\n",
      "Index: []\n",
      "industrial_acad\n",
      "Empty DataFrame\n",
      "Columns: [acc_type, words, count]\n",
      "Index: []\n",
      "industrial_gov\n",
      "   acc_type          words  count\n",
      "0       gov          april      1\n",
      "14      gov       maintain      1\n",
      "25      gov          stack      1\n",
      "24      gov          sewer      1\n",
      "23      gov        service      1\n",
      "22      gov        problem      1\n",
      "21      gov          point      1\n",
      "20      gov          piney      1\n",
      "19      gov  phosphogypsum      1\n",
      "18      gov      phosphate      1\n",
      "industrial_media\n",
      "Empty DataFrame\n",
      "Columns: [acc_type, words, count]\n",
      "Index: []\n",
      "industrial_other\n",
      "Empty DataFrame\n",
      "Columns: [acc_type, words, count]\n",
      "Index: []\n",
      "industrial_tourbiz\n",
      "Empty DataFrame\n",
      "Columns: [acc_type, words, count]\n",
      "Index: []\n",
      "sewage_acad\n",
      "   acc_type          words  count\n",
      "31     acad          water      2\n",
      "1      acad           area      2\n",
      "30     acad           time      1\n",
      "29     acad           term      1\n",
      "28     acad           talk      1\n",
      "27     acad       surround      1\n",
      "26     acad            sho      1\n",
      "25     acad      saltwater      1\n",
      "24     acad         runoff      1\n",
      "23     acad  radioactivity      1\n",
      "sewage_gov\n",
      "   acc_type    words  count\n",
      "2       gov     area      2\n",
      "51      gov     team      2\n",
      "0       gov  alabama      1\n",
      "45      gov   repair      1\n",
      "33      gov    mayor      1\n",
      "34      gov      mid      1\n",
      "35      gov    multi      1\n",
      "36      gov    nasty      1\n",
      "37      gov    night      1\n",
      "38      gov  nokomis      1\n",
      "sewage_media\n",
      "   acc_type       words  count\n",
      "91    media       water      6\n",
      "54    media        late      4\n",
      "24    media    continue      4\n",
      "18    media        city      3\n",
      "42    media      gallon      3\n",
      "22    media     confirm      2\n",
      "76    media        rise      2\n",
      "52    media       issue      2\n",
      "27    media  correction      2\n",
      "31    media       crest      2\n",
      "sewage_other\n",
      "    acc_type           words  count\n",
      "360    other          gallon     49\n",
      "952    other           water     48\n",
      "644    other            pipe     22\n",
      "529    other         manatee     19\n",
      "148    other            city     18\n",
      "430    other           human     16\n",
      "987    other            year     15\n",
      "950    other           waste     15\n",
      "454    other  infrastructure     15\n",
      "865    other            swim     14\n",
      "sewage_tourbiz\n",
      "  acc_type        words  count\n",
      "0  tourbiz        await      1\n",
      "1  tourbiz  conjunction      1\n",
      "2  tourbiz          day      1\n",
      "3  tourbiz         hole      1\n",
      "4  tourbiz        issue      1\n",
      "5  tourbiz         late      1\n",
      "6  tourbiz    literally      1\n",
      "7  tourbiz        rapid      1\n",
      "8  tourbiz       region      1\n",
      "9  tourbiz         shit      1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v1/dkrd1tzx2cvd0z9f5pbj8xp00000gn/T/ipykernel_35568/1421598334.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  file_acc_typed['words'] = file_acc_typed['text_with_display_links'].str.split()\n",
      "/var/folders/v1/dkrd1tzx2cvd0z9f5pbj8xp00000gn/T/ipykernel_35568/1421598334.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  file_acc_typed['words'] = file_acc_typed['text_with_display_links'].str.split()\n",
      "/var/folders/v1/dkrd1tzx2cvd0z9f5pbj8xp00000gn/T/ipykernel_35568/1421598334.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  file_acc_typed['words'] = file_acc_typed['text_with_display_links'].str.split()\n",
      "/var/folders/v1/dkrd1tzx2cvd0z9f5pbj8xp00000gn/T/ipykernel_35568/1421598334.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  file_acc_typed['words'] = file_acc_typed['text_with_display_links'].str.split()\n",
      "/var/folders/v1/dkrd1tzx2cvd0z9f5pbj8xp00000gn/T/ipykernel_35568/1421598334.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  file_acc_typed['words'] = file_acc_typed['text_with_display_links'].str.split()\n"
     ]
    }
   ],
   "source": [
    "tf_idf_preprocessing(oil_w_acc, \"acad\", oil_stop_words, locations, \"oil_acad\")\n",
    "tf_idf_preprocessing(oil_w_acc, \"gov\", oil_stop_words, locations, \"oil_gov\")\n",
    "tf_idf_preprocessing(oil_w_acc, \"media\", oil_stop_words, locations, \"oil_media\")\n",
    "tf_idf_preprocessing(oil_w_acc, \"other\", oil_stop_words, locations, \"oil_other\")\n",
    "tf_idf_preprocessing(oil_w_acc, \"tourbiz\", oil_stop_words, locations, \"oil_tourbiz\")\n",
    "tf_idf_preprocessing(industrial_w_acc, \"acad\", industrial_stop_words, locations, \"industrial_acad\")\n",
    "tf_idf_preprocessing(industrial_w_acc, \"gov\", industrial_stop_words, locations, \"industrial_gov\")\n",
    "tf_idf_preprocessing(industrial_w_acc, \"media\", industrial_stop_words, locations, \"industrial_media\")\n",
    "tf_idf_preprocessing(industrial_w_acc, \"other\", industrial_stop_words, locations, \"industrial_other\")\n",
    "tf_idf_preprocessing(industrial_w_acc, \"tourbiz\", industrial_stop_words, locations, \"industrial_tourbiz\")\n",
    "tf_idf_preprocessing(sewage_w_acc, \"acad\", sewage_stop_words, locations, \"sewage_acad\")\n",
    "tf_idf_preprocessing(sewage_w_acc, \"gov\", sewage_stop_words, locations, \"sewage_gov\")\n",
    "tf_idf_preprocessing(sewage_w_acc, \"media\", sewage_stop_words, locations, \"sewage_media\")\n",
    "tf_idf_preprocessing(sewage_w_acc, \"other\", sewage_stop_words, locations, \"sewage_other\")\n",
    "tf_idf_preprocessing(sewage_w_acc, \"tourbiz\", sewage_stop_words, locations, \"sewage_tourbiz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8e56dc17-118b-403e-a12d-3f25512a20c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "654    tampa bay leaking oil\n",
      "Name: text_with_display_links, dtype: object\n"
     ]
    }
   ],
   "source": [
    "oil_w_acc = oil_w_acc[oil_w_acc[\"hand.label_simplified\"]==\"tourbiz\"]\n",
    "print(oil_w_acc[\"text_with_display_links\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "92dc19d9-663a-4eef-9cd0-4510f3825947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2256    grease nasty problem block sewer line wastewat...\n",
      "8303    gallon wastewater piney point phosphate mining...\n",
      "Name: text_with_display_links, dtype: object\n"
     ]
    }
   ],
   "source": [
    "industrial_w_acc = industrial_w_acc[industrial_w_acc[\"hand.label_simplified\"]==\"gov\"]\n",
    "print(industrial_w_acc[\"text_with_display_links\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "eb198229-890c-48b0-a425-b3f2f06c3132",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# merging all dynamically\n",
    "files = [os.path.join(\"not_merged_tf_idf\", file) for file in os.listdir(\"not_merged_tf_idf\") if file.endswith('.csv')]\n",
    "\n",
    "# List to hold all the DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Read each file into a DataFrame and add it to the list\n",
    "for file in files:\n",
    "    df = pd.read_csv(file)  # adjust this line based on your file format\n",
    "    dfs.append(df)\n",
    "\n",
    "# Merge all DataFrames in the list\n",
    "merged_df = dfs[0]\n",
    "for df in dfs[1:]:\n",
    "    merged_df = merged_df.merge(df, how='outer')  # Adjust 'common_column' and 'how' as needed\n",
    "\n",
    "# Save the merged DataFrame to a new CSV file\n",
    "merged_df.to_csv('merged_tf_idf/tf_idf_preprocessed.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50696b1a-a16a-4207-a72f-7c298c9cd062",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# TF-IDF WordCloud Incomplete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "76c5e41e-0684-475a-979a-a214ec8bfd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_preprocessed = pd.read_csv('merged_tf_idf/tf_idf_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3b4fd502-b7d1-4968-9b4c-59d361774634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resource: https://ayselaydin.medium.com/6-creating-a-word-cloud-using-tf-idf-in-python-2554742d86d9 (i believe she's turkish ;))\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "tfidf_matrix = vectorizer.fit_transform(tf_idf_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "aa56d8f3-4c9b-409b-b555-5591fac41ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 3 stored elements and shape (3, 3)>\n",
      "  Coords\tValues\n",
      "  (0, 0)\t1.0\n",
      "  (1, 2)\t1.0\n",
      "  (2, 1)\t1.0\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "f66cb031-dfa1-4d81-85ab-65cf726d6d80",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'WordCloud' from 'wordcloud' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[112], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m wordcloud \u001b[38;5;241m=\u001b[39m WordCloud(width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m800\u001b[39m, height\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m400\u001b[39m, background_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwhite\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mgenerate_from_frequencies(vectorizer\u001b[38;5;241m.\u001b[39mvocabulary_)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'WordCloud' from 'wordcloud' (unknown location)"
     ]
    }
   ],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(vectorizer.vocabulary_)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b600b3e8-b411-4650-bcf8-343e66e9ff8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "e37f6f24e9a9f73ff6720cc463cd31ad7ef16f22c85d3da331e44c0f6e80d360"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
