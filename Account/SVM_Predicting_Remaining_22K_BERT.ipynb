{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fc626fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             username                                        description  \\\n",
      "0       ChungSunPark4                                      attempt me!!!   \n",
      "1        LucilaQuanti  Me gusta la gente con sentido del humor, alegr...   \n",
      "2       patdefranchis  I love people with a large dose of humor & a r...   \n",
      "3       saravastiares  Have courage & Be Kind. Where there is kindnes...   \n",
      "4      TheShogunGamer  Video Game Extraordinaire 🎮 Polyamorous ❤ Disa...   \n",
      "...               ...                                                ...   \n",
      "3493   ChrisFischer07                                                NaN   \n",
      "3494       ryantpa813  Eight One Three Sports\\nBolts, Bucs, Rays, Soo...   \n",
      "3495  TueNiteRockStar                                                NaN   \n",
      "3496     purpletang99  i tweet my opinions only. yes there are except...   \n",
      "3497       firefly909  Unapologetic bleeding heart liberal. Hate liar...   \n",
      "\n",
      "                                 description_lemmatized  \n",
      "0                      ['attempt', 'me', '!', '!', '!']  \n",
      "1     ['me', 'gusta', 'la', 'gente', 'con', 'sentido...  \n",
      "2     ['i', 'love', 'people', 'with', 'a', 'large', ...  \n",
      "3     ['have', 'courage', '&', 'be', 'kind', '.', 'w...  \n",
      "4     ['video', 'game', 'extraordinaire', '🎮', 'poly...  \n",
      "...                                                 ...  \n",
      "3493                                                     \n",
      "3494  ['eight', 'one', 'three', 'sport', 'bolt', ','...  \n",
      "3495                                                     \n",
      "3496  ['i', 'tweet', 'my', 'opinion', 'only', '.', '...  \n",
      "3497  ['unapologetic', 'bleed', 'heart', 'liberal', ...  \n",
      "\n",
      "[3498 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_predict, train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import json\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pickle\n",
    "\n",
    "tag_map = defaultdict(lambda: wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV\n",
    "tag_map['AS'] = wn.ADJ_SAT\n",
    "\n",
    "# filepath = \"finalized_8K_accounts.csv\"\n",
    "# filepath = \"UNLABELED_accounts_emojis_replaced.csv\"\n",
    "filepath = \"Spill_Accounts_To_Be_Labeled.csv\"\n",
    "hand_label = \"hand.label\"\n",
    "government = \"gov\"\n",
    "academia = \"acad\"\n",
    "tourBiz = \"tourbiz\"\n",
    "\n",
    "df = pd.read_csv(filepath)\n",
    "\n",
    "# df = df[((df[hand_label] == 'media') | (df[hand_label] == tourBiz) |(df[hand_label] == academia) | (df[hand_label] == government) | (\n",
    "#        df[hand_label] == 'other'))]\n",
    "\n",
    "df = df[['username', 'description']]  # keep only relevant columns\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words_not_changed = ['media']\n",
    "\n",
    "\n",
    "def preprocessing(row):\n",
    "    if str(row) == \"nan\":\n",
    "        lemma = \"\"\n",
    "    else:\n",
    "        row = str(row).lower()\n",
    "        row = word_tokenize(row)  # tokenize\n",
    "        lemma = [lemmatizer.lemmatize(token, tag_map[tag[0]]) if token not in words_not_changed else token for\n",
    "                 token, tag in pos_tag(row)]  # lemmatization, depending on part-of-speech\n",
    "        lemma = [\"\" if re.search(r'\\b[0-9]+\\b\\s*', lem) else lem for lem in lemma]  # removing\n",
    "    return str(lemma)\n",
    "\n",
    "\n",
    "df['description_lemmatized'] = df['description'].apply(preprocessing)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8b4d3f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3498, 3)\n",
      "(2908, 3)\n"
     ]
    }
   ],
   "source": [
    "# all the empty descriptions\n",
    "print(df.shape)\n",
    "print(df[df['description_lemmatized'] != \"\"].shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "57a23e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(590, 3)\n",
      "(2908, 3)\n"
     ]
    }
   ],
   "source": [
    "# Remove all the empty descriptions\n",
    "empty_rows = df[df['description_lemmatized'] == \"\"]\n",
    "print(empty_rows.shape)\n",
    "df = df[df['description_lemmatized'] != \"\"]\n",
    "print(df.shape)\n",
    "#df[hand_label]\n",
    "#print(df.shape)\n",
    "#df[df['description_lemmatized'] != \"\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f7612c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-indexing the remaining observations\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "2af8cfdb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Tensor' from 'torch' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[145], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m SentenceTransformer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall-MiniLM-L6-v2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# print(type(df[['description_lemmatized']]))\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sentence_transformers/__init__.py:7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimportlib\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcross_encoder\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mCrossEncoder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrossEncoder\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ParallelSentencesDataset, SentencesDataset\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mLoggingHandler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LoggingHandler\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sentence_transformers/cross_encoder/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mCrossEncoder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrossEncoder\n\u001b[1;32m      3\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCrossEncoder\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tensor, nn\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Optimizer\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Tensor' from 'torch' (unknown location)"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# print(type(df[['description_lemmatized']]))\n",
    "embeddings = model.encode(df['description'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13dfb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = 'SVM_BOW_unweighted_enhanced_model.pickle'\n",
    "filename = 'SVM_BERT_unweighted_enhanced_model_full(1, 2).pickle'\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "X_test = embeddings\n",
    "\n",
    "bag_of_words_y_pred_test = loaded_model.predict(X_test)\n",
    "\n",
    "bag_of_words_y_pred_test\n",
    "\n",
    "pred_prob = loaded_model.predict_proba(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1d7478",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_prob\n",
    "bag_of_words_y_pred_test\n",
    "pd.concat([pd.DataFrame(bag_of_words_y_pred_test), pd.DataFrame(pred_prob)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae85ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_prob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd3b9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_prob_df = pd.DataFrame(pred_prob, columns = ['acad_prob','gov_prob','media_prob','other_prob', 'tourbiz_prob'])\n",
    "\n",
    "bag_of_words_y_pred_test.size\n",
    "\n",
    "df['hand.label_simplified'] = bag_of_words_y_pred_test\n",
    "#df = df.drop(columns=['description_lemmatized'])\n",
    "df1 = pd.concat([df, pred_prob_df], axis=1)\n",
    "#df1 = pd.DataFrame(my_array, columns = ['acad_prob','gov_prob','media_prob','other_prob', 'tourbiz_prob'])\n",
    "\n",
    "df1\n",
    "df1.shape\n",
    "#pred_prob_df.shape\n",
    "#len(bag_of_words_y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "8b0141ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[151], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df1\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df1' is not defined"
     ]
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5f0ec6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv(r'SVM_BERT_unweighted_UNLABELED_PREDICTED_accounts_W_PROBABILITIES_emojis_unchanged.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e1ce37dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "spill_labeled_accs = pd.read_csv(\"Spill_Labeled.csv\")\n",
    "#spill_labeled_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c4ea2422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26172\n",
      "hand.label_simplified\n",
      "other      2680\n",
      "media       162\n",
      "acad         62\n",
      "tourbiz       3\n",
      "gov           1\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "hand.label_simplified\n",
       "other      0.921596\n",
       "media      0.055708\n",
       "acad       0.021320\n",
       "tourbiz    0.001032\n",
       "gov        0.000344\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(spill_labeled_accs.size)\n",
    "\n",
    "print(spill_labeled_accs[\"hand.label_simplified\"].value_counts())\n",
    "\n",
    "spill_labeled_accs[\"hand.label_simplified\"].value_counts(\"row\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "90e98606-aa67-431b-b022-a4de1008c18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the rows labeled as other\n",
    "mask = spill_labeled_accs[\"hand.label_simplified\"] == 'other'\n",
    "\n",
    "\n",
    "row_list = []\n",
    "# Check if any of the probabilities (except 'other_prob') are greater than 0.3\n",
    "for index, row in spill_labeled_accs[mask].iterrows():\n",
    "    for col in ['acad_prob', 'gov_prob', 'media_prob', 'other_prob', 'tourbiz_prob']:\n",
    "        if col != 'other_prob' and row[col] > 0.3:\n",
    "            # saving the columns to review\n",
    "            row_list.append(row)\n",
    "            # Update the prediction column to the column name where the probability is higher than 0.3\n",
    "            #spill_labeled_accs.at[index, \"hand.label_simplified\"] = col.replace('_prob', '')\n",
    "rows_for_review = pd.DataFrame(row_list)\n",
    "rows_for_review = rows_for_review.drop_duplicates()\n",
    "#print(rows_for_review)\n",
    "\n",
    "rows_for_review.to_csv(\"Accounts_To_Relabel.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7e70eb64-18a4-4ac1-9b23-4c02e05c8006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hand.label_simplified\n",
      "other      2680\n",
      "media       162\n",
      "acad         62\n",
      "tourbiz       3\n",
      "gov           1\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>description</th>\n",
       "      <th>description_lemmatized</th>\n",
       "      <th>hand.label_simplified</th>\n",
       "      <th>acad_prob</th>\n",
       "      <th>gov_prob</th>\n",
       "      <th>media_prob</th>\n",
       "      <th>other_prob</th>\n",
       "      <th>tourbiz_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ChungSunPark4</td>\n",
       "      <td>attempt me!!!</td>\n",
       "      <td>['attempt', 'me', '!', '!', '!']</td>\n",
       "      <td>other</td>\n",
       "      <td>0.000776</td>\n",
       "      <td>0.000603</td>\n",
       "      <td>0.004676</td>\n",
       "      <td>0.993783</td>\n",
       "      <td>0.000162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LucilaQuanti</td>\n",
       "      <td>Me gusta la gente con sentido del humor, alegr...</td>\n",
       "      <td>['me', 'gusta', 'la', 'gente', 'con', 'sentido...</td>\n",
       "      <td>other</td>\n",
       "      <td>0.009851</td>\n",
       "      <td>0.001305</td>\n",
       "      <td>0.006343</td>\n",
       "      <td>0.981409</td>\n",
       "      <td>0.001092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>patdefranchis</td>\n",
       "      <td>I love people with a large dose of humor &amp; a r...</td>\n",
       "      <td>['i', 'love', 'people', 'with', 'a', 'large', ...</td>\n",
       "      <td>other</td>\n",
       "      <td>0.003543</td>\n",
       "      <td>0.000746</td>\n",
       "      <td>0.004899</td>\n",
       "      <td>0.990260</td>\n",
       "      <td>0.000552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>saravastiares</td>\n",
       "      <td>Have courage &amp; Be Kind. Where there is kindnes...</td>\n",
       "      <td>['have', 'courage', '&amp;', 'be', 'kind', '.', 'w...</td>\n",
       "      <td>other</td>\n",
       "      <td>0.003940</td>\n",
       "      <td>0.000537</td>\n",
       "      <td>0.007693</td>\n",
       "      <td>0.985011</td>\n",
       "      <td>0.002819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TheShogunGamer</td>\n",
       "      <td>Video Game Extraordinaire 🎮 Polyamorous ❤ Disa...</td>\n",
       "      <td>['video', 'game', 'extraordinaire', '🎮', 'poly...</td>\n",
       "      <td>other</td>\n",
       "      <td>0.000925</td>\n",
       "      <td>0.000598</td>\n",
       "      <td>0.003607</td>\n",
       "      <td>0.994229</td>\n",
       "      <td>0.000641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2903</th>\n",
       "      <td>meevans59</td>\n",
       "      <td>Retired Department of Defense civilian employe...</td>\n",
       "      <td>['retired', 'department', 'of', 'defense', 'ci...</td>\n",
       "      <td>other</td>\n",
       "      <td>0.001988</td>\n",
       "      <td>0.004162</td>\n",
       "      <td>0.002987</td>\n",
       "      <td>0.990808</td>\n",
       "      <td>0.000055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2904</th>\n",
       "      <td>dvdhnz</td>\n",
       "      <td>Techie by trade, recalcitrant by design, socia...</td>\n",
       "      <td>['techie', 'by', 'trade', ',', 'recalcitrant',...</td>\n",
       "      <td>other</td>\n",
       "      <td>0.015865</td>\n",
       "      <td>0.002104</td>\n",
       "      <td>0.001636</td>\n",
       "      <td>0.978123</td>\n",
       "      <td>0.002272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2905</th>\n",
       "      <td>ryantpa813</td>\n",
       "      <td>Eight One Three Sports\\nBolts, Bucs, Rays, Soo...</td>\n",
       "      <td>['eight', 'one', 'three', 'sport', 'bolt', ','...</td>\n",
       "      <td>other</td>\n",
       "      <td>0.000745</td>\n",
       "      <td>0.001396</td>\n",
       "      <td>0.033677</td>\n",
       "      <td>0.962982</td>\n",
       "      <td>0.001200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2906</th>\n",
       "      <td>purpletang99</td>\n",
       "      <td>i tweet my opinions only. yes there are except...</td>\n",
       "      <td>['i', 'tweet', 'my', 'opinion', 'only', '.', '...</td>\n",
       "      <td>other</td>\n",
       "      <td>0.003356</td>\n",
       "      <td>0.000360</td>\n",
       "      <td>0.076140</td>\n",
       "      <td>0.919635</td>\n",
       "      <td>0.000509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2907</th>\n",
       "      <td>firefly909</td>\n",
       "      <td>Unapologetic bleeding heart liberal. Hate liar...</td>\n",
       "      <td>['unapologetic', 'bleed', 'heart', 'liberal', ...</td>\n",
       "      <td>other</td>\n",
       "      <td>0.000927</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000698</td>\n",
       "      <td>0.998056</td>\n",
       "      <td>0.000247</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2908 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            username                                        description  \\\n",
       "0      ChungSunPark4                                      attempt me!!!   \n",
       "1       LucilaQuanti  Me gusta la gente con sentido del humor, alegr...   \n",
       "2      patdefranchis  I love people with a large dose of humor & a r...   \n",
       "3      saravastiares  Have courage & Be Kind. Where there is kindnes...   \n",
       "4     TheShogunGamer  Video Game Extraordinaire 🎮 Polyamorous ❤ Disa...   \n",
       "...              ...                                                ...   \n",
       "2903       meevans59  Retired Department of Defense civilian employe...   \n",
       "2904          dvdhnz  Techie by trade, recalcitrant by design, socia...   \n",
       "2905      ryantpa813  Eight One Three Sports\\nBolts, Bucs, Rays, Soo...   \n",
       "2906    purpletang99  i tweet my opinions only. yes there are except...   \n",
       "2907      firefly909  Unapologetic bleeding heart liberal. Hate liar...   \n",
       "\n",
       "                                 description_lemmatized hand.label_simplified  \\\n",
       "0                      ['attempt', 'me', '!', '!', '!']                 other   \n",
       "1     ['me', 'gusta', 'la', 'gente', 'con', 'sentido...                 other   \n",
       "2     ['i', 'love', 'people', 'with', 'a', 'large', ...                 other   \n",
       "3     ['have', 'courage', '&', 'be', 'kind', '.', 'w...                 other   \n",
       "4     ['video', 'game', 'extraordinaire', '🎮', 'poly...                 other   \n",
       "...                                                 ...                   ...   \n",
       "2903  ['retired', 'department', 'of', 'defense', 'ci...                 other   \n",
       "2904  ['techie', 'by', 'trade', ',', 'recalcitrant',...                 other   \n",
       "2905  ['eight', 'one', 'three', 'sport', 'bolt', ','...                 other   \n",
       "2906  ['i', 'tweet', 'my', 'opinion', 'only', '.', '...                 other   \n",
       "2907  ['unapologetic', 'bleed', 'heart', 'liberal', ...                 other   \n",
       "\n",
       "      acad_prob  gov_prob  media_prob  other_prob  tourbiz_prob  \n",
       "0      0.000776  0.000603    0.004676    0.993783      0.000162  \n",
       "1      0.009851  0.001305    0.006343    0.981409      0.001092  \n",
       "2      0.003543  0.000746    0.004899    0.990260      0.000552  \n",
       "3      0.003940  0.000537    0.007693    0.985011      0.002819  \n",
       "4      0.000925  0.000598    0.003607    0.994229      0.000641  \n",
       "...         ...       ...         ...         ...           ...  \n",
       "2903   0.001988  0.004162    0.002987    0.990808      0.000055  \n",
       "2904   0.015865  0.002104    0.001636    0.978123      0.002272  \n",
       "2905   0.000745  0.001396    0.033677    0.962982      0.001200  \n",
       "2906   0.003356  0.000360    0.076140    0.919635      0.000509  \n",
       "2907   0.000927  0.000073    0.000698    0.998056      0.000247  \n",
       "\n",
       "[2908 rows x 9 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(spill_labeled_accs[\"hand.label_simplified\"].value_counts())\n",
    "\n",
    "spill_labeled_accs[\"hand.label_simplified\"].value_counts(\"row\")\n",
    "spill_labeled_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b534871b-8605-4fb8-ba09-4e9c07c4f346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hand.label_simplified\n",
       "other      2606\n",
       "media       162\n",
       "acad         62\n",
       "other        36\n",
       "media        17\n",
       "acad         14\n",
       "gov           5\n",
       "tourbiz       3\n",
       "tourbiz       2\n",
       "gov           1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## replacing some rows with their manually labeled values\n",
    "\n",
    "relabeled = pd.read_csv(\"Final_Account_Relabeling.csv\")\n",
    "mask = spill_labeled_accs[\"username\"].isin(relabeled[\"username\"])\n",
    "# Update columns in df1 with corresponding values from df2 where usernames match\n",
    "for index, row in spill_labeled_accs[mask].iterrows():\n",
    "    relabel_row = relabeled[relabeled['username'] == row['username']]\n",
    "    spill_labeled_accs.loc[index, 'hand.label_simplified'] = relabel_row['hand.label_simplified'].values\n",
    "spill_labeled_accs[\"hand.label_simplified\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "65319d29-2b4b-4cf7-a9ac-941aa25480d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3498, 9)\n",
      "             username                                        description  \\\n",
      "0             004nino  Retraité.Égalité,fraternité,justice,liberté,na...   \n",
      "1              00a03d  Mom of one, grandma of two, sister of six, aun...   \n",
      "2     05adamlover0129  LOVE Adam Lambert for life!!! Also love Miami ...   \n",
      "3     0Plongstocking2  Not a traitor. Traveler. Liker of food. Orchid...   \n",
      "4             0Thessa  💚... schönheit\\nist die natur! die kunst ist u...   \n",
      "...               ...                                                ...   \n",
      "3493   zoeycarmicheal  The Univ. of Alabama Alumni. Former Gymnast an...   \n",
      "3494           zpleat  Research at Media Matters for America. All twe...   \n",
      "3495         zshahan3  Human (maybe), writer + chief editor + CEO @Cl...   \n",
      "3496          zul1732  You can't see people for what they are when yo...   \n",
      "3497        zyiteblog  E-Scootersworld is global Power transport reta...   \n",
      "\n",
      "                                 description_lemmatized hand.label_simplified  \\\n",
      "0     ['retraité.égalité', ',', 'fraternité', ',', '...                 other   \n",
      "1     ['mom', 'of', 'one', ',', 'grandma', 'of', 'tw...                 other   \n",
      "2     ['love', 'adam', 'lambert', 'for', 'life', '!'...                 other   \n",
      "3     ['not', 'a', 'traitor', '.', 'traveler', '.', ...                 other   \n",
      "4     ['💚', '...', 'schönheit', 'ist', 'die', 'natur...                 other   \n",
      "...                                                 ...                   ...   \n",
      "3493  ['the', 'univ', '.', 'of', 'alabama', 'alumnus...                 other   \n",
      "3494  ['research', 'at', 'media', 'matter', 'for', '...                 other   \n",
      "3495  ['human', '(', 'maybe', ')', ',', 'writer', '+...                 media   \n",
      "3496  ['you', 'ca', \"n't\", 'see', 'people', 'for', '...                 other   \n",
      "3497  ['e-scootersworld', 'be', 'global', 'power', '...                 other   \n",
      "\n",
      "      acad_prob  gov_prob  media_prob  other_prob  tourbiz_prob  \n",
      "0      0.017303  0.002636    0.010321    0.969045      0.000695  \n",
      "1      0.008145  0.002770    0.011733    0.977149      0.000203  \n",
      "2      0.005680  0.000770    0.202065    0.788418      0.003066  \n",
      "3      0.006012  0.005455    0.001879    0.980456      0.006198  \n",
      "4      0.014347  0.017324    0.018455    0.948339      0.001535  \n",
      "...         ...       ...         ...         ...           ...  \n",
      "3493   0.003370  0.003790    0.041372    0.948383      0.003084  \n",
      "3494   0.031496  0.002862    0.050988    0.914259      0.000394  \n",
      "3495   0.004032  0.000303    0.693154    0.302204      0.000308  \n",
      "3496   0.005312  0.000945    0.001609    0.986842      0.005293  \n",
      "3497   0.005406  0.005807    0.019533    0.938352      0.030902  \n",
      "\n",
      "[3498 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "spill_accs_merged = pd.merge(spill_labeled_accs, empty_rows, how=\"outer\")\n",
    "\n",
    "print(spill_accs_merged.shape)\n",
    "print(spill_accs_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0c61f255-89d1-455c-b08b-cf584df4fe9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              username                                        description  \\\n",
      "0                  CNN  It’s our job to #GoThere & tell the most diffi...   \n",
      "1               NatGeo  Taking our understanding and awareness of the ...   \n",
      "2              FoxNews  Follow America's #1 cable news network, delive...   \n",
      "3       washingtonpost                         Democracy Dies in Darkness   \n",
      "4                  ABC  The only official ABC News Twitter account. Do...   \n",
      "...                ...                                                ...   \n",
      "29128  EvergreenZephyr  Wichita, Kansas, United (sic) States. Parody a...   \n",
      "29129         johntfox  Madeleine & Marin's Dad | Gin Enthusiast | Twe...   \n",
      "29130         SeGreene  Cranky former nurse and current plant patholog...   \n",
      "29131      CherylLasse  Passionate about the environment, science and ...   \n",
      "29132          jen_pic  🚫socialism. Pay your debts, ALL OF THEM! Nothi...   \n",
      "\n",
      "       Label Label.Type  \n",
      "0      media       Hand  \n",
      "1      media       Hand  \n",
      "2      media       Hand  \n",
      "3      media       Hand  \n",
      "4      media       Hand  \n",
      "...      ...        ...  \n",
      "29128  other    Predict  \n",
      "29129  other    Predict  \n",
      "29130  other    Predict  \n",
      "29131  other    Predict  \n",
      "29132  other    Predict  \n",
      "\n",
      "[29133 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# acccounts labeled during Red-Tide research\n",
    "prev_accs = pd.read_csv(\"Final_Account_Labels_for_Dashboard.csv\")\n",
    "print(prev_accs)\n",
    "accounts_merged = pd.concat([spill_accs_merged, prev_accs], ignore_index=True)\n",
    "\n",
    "accounts_merged.to_csv(\"ALL_Labeled_Accounts_Spill&RedTide.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "28303242-c700-4917-b0b4-4f62a86581cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hand.label_simplified\n",
      "other      247\n",
      "media       31\n",
      "acad        19\n",
      "media        7\n",
      "acad         3\n",
      "other        2\n",
      "gov          1\n",
      "tourbiz      1\n",
      "Name: count, dtype: int64\n",
      "hand.label_simplified\n",
      "other      0.794212\n",
      "media      0.099678\n",
      "acad       0.061093\n",
      "media      0.022508\n",
      "acad       0.009646\n",
      "other      0.006431\n",
      "gov        0.003215\n",
      "tourbiz    0.003215\n",
      "Name: proportion, dtype: float64\n",
      "hand.label_simplified\n",
      "other      207\n",
      "media        9\n",
      "acad         4\n",
      "other        3\n",
      "gov          3\n",
      "media        2\n",
      "tourbiz      1\n",
      "gov          1\n",
      "Name: count, dtype: int64\n",
      "hand.label_simplified\n",
      "other      0.900000\n",
      "media      0.039130\n",
      "acad       0.017391\n",
      "other      0.013043\n",
      "gov        0.013043\n",
      "media      0.008696\n",
      "tourbiz    0.004348\n",
      "gov        0.004348\n",
      "Name: proportion, dtype: float64\n",
      "hand.label_simplified\n",
      "other      2179\n",
      "media       127\n",
      "acad         39\n",
      "other        31\n",
      "acad         11\n",
      "media         9\n",
      "tourbiz       2\n",
      "tourbiz       1\n",
      "gov           1\n",
      "gov           1\n",
      "Name: count, dtype: int64\n",
      "hand.label_simplified\n",
      "other      0.907539\n",
      "media      0.052895\n",
      "acad       0.016243\n",
      "other      0.012911\n",
      "acad       0.004581\n",
      "media      0.003748\n",
      "tourbiz    0.000833\n",
      "tourbiz    0.000416\n",
      "gov        0.000416\n",
      "gov        0.000416\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "oil = pd.read_csv(\"../spill_data/Cleaned_Files/C_All_Oil.csv\")\n",
    "sewage = pd.read_csv(\"../spill_data/Cleaned_Files/C_All_Sewage.csv\")\n",
    "industrial = pd.read_csv(\"../spill_data/Cleaned_Files/C_All_Industrial.csv\")\n",
    "\n",
    "oil_accs = accounts_merged[accounts_merged[\"username\"].isin(oil[\"username\"])]\n",
    "print(oil_accs[\"hand.label_simplified\"].value_counts())\n",
    "print(oil_accs[\"hand.label_simplified\"].value_counts(\"row\"))\n",
    "\n",
    "sewage_accs = accounts_merged[accounts_merged[\"username\"].isin(sewage[\"username\"])]\n",
    "print(sewage_accs[\"hand.label_simplified\"].value_counts())\n",
    "print(sewage_accs[\"hand.label_simplified\"].value_counts(\"row\"))\n",
    "\n",
    "industrial_accs = accounts_merged[accounts_merged[\"username\"].isin(industrial[\"username\"])]\n",
    "print(industrial_accs[\"hand.label_simplified\"].value_counts())\n",
    "print(industrial_accs[\"hand.label_simplified\"].value_counts(\"row\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9949f866-2efb-4a67-a2c8-e4c4df85e92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf-idf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d54a295b-f43c-4544-b04b-20289b2f4a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Unnamed: 0                                               text  \\\n",
      "0             1  Reposting @divyendhu:\\nOil Spill on the Gulf o...   \n",
      "1             2  RT @hihi0806: Reposting @divyendhu:\\nOil Spill...   \n",
      "2             3  RT @hihi0806: Reposting @divyendhu:\\nOil Spill...   \n",
      "3             4  RT @hihi0806: Reposting @divyendhu:\\nOil Spill...   \n",
      "4             5  RT @hihi0806: Reposting @divyendhu:\\nOil Spill...   \n",
      "..          ...                                                ...   \n",
      "705         615  @1053SS I think this team shouldn’t even be ov...   \n",
      "706           1  The governor is responsible, but those who cam...   \n",
      "707           2  The governor is responsible, but those who cam...   \n",
      "708           3  The governor is responsible, but those who cam...   \n",
      "709           4  RT @DGRFlorida: The governor is responsible, b...   \n",
      "\n",
      "     possibly_sensitive                   id            author_id  \\\n",
      "0                  True  1079362755352817665             95188073   \n",
      "1                 False  1079363436130136069  1053787947902758918   \n",
      "2                 False  1079462270374215681            842498792   \n",
      "3                 False  1342684372009447424           2241627420   \n",
      "4                 False  1342687535093473286            146645976   \n",
      "..                  ...                  ...                  ...   \n",
      "705               False  1604784465686827009           2419893955   \n",
      "706               False  1033090028497321984           3384569867   \n",
      "707               False  1033091674820423681            175569476   \n",
      "708               False  1033092550393921536   773350091440680960   \n",
      "709               False  1039072225054392320             29393077   \n",
      "\n",
      "         conversation_id  in_reply_to_user_id reply_settings lang  \\\n",
      "0    1079362755352817665                  NaN       everyone   en   \n",
      "1    1079363436130136069                  NaN       everyone   en   \n",
      "2    1079462270374215681                  NaN       everyone   en   \n",
      "3    1342684372009447424                  NaN       everyone   en   \n",
      "4    1342687535093473286                  NaN       everyone   en   \n",
      "..                   ...                  ...            ...  ...   \n",
      "705  1604668490555629568          132734157.0       everyone   en   \n",
      "706  1033090028497321984                  NaN       everyone   en   \n",
      "707  1033091674820423681                  NaN       everyone   en   \n",
      "708  1033092550393921536                  NaN       everyone   en   \n",
      "709  1039072225054392320                  NaN       everyone   en   \n",
      "\n",
      "            created_at.x  ...  \\\n",
      "0    2018-12-30 13:05:09  ...   \n",
      "1    2018-12-30 13:07:51  ...   \n",
      "2    2018-12-30 19:40:35  ...   \n",
      "3    2020-12-26 04:11:14  ...   \n",
      "4    2020-12-26 04:23:48  ...   \n",
      "..                   ...  ...   \n",
      "705  2022-12-19 10:23:26  ...   \n",
      "706  2018-08-24 20:33:51  ...   \n",
      "707  2018-08-24 20:40:23  ...   \n",
      "708  2018-08-24 20:43:52  ...   \n",
      "709  2018-09-10 08:44:58  ...   \n",
      "\n",
      "                                         description_y  \\\n",
      "0    Hi!! ☕️Good to see you!! 🍵Please be sure to en...   \n",
      "1                                        attempt me!!!   \n",
      "2    Me gusta la gente con sentido del humor, alegr...   \n",
      "3    I love people with a large dose of humor & a r...   \n",
      "4    Have courage & Be Kind. Where there is kindnes...   \n",
      "..                                                 ...   \n",
      "705  The sun never says to earth you owe me for shi...   \n",
      "706  Official Deep Green Resistance Florida! You ca...   \n",
      "707                                                NaN   \n",
      "708  Fighting Pipelines Everywhere! #waterprotector...   \n",
      "709  Up The Irons🤘System Change #NotMeUs NoLISTS #H...   \n",
      "\n",
      "                                description_lemmatized  hand.label_simplified  \\\n",
      "0                                                  NaN                    NaN   \n",
      "1                     ['attempt', 'me', '!', '!', '!']                  other   \n",
      "2    ['me', 'gusta', 'la', 'gente', 'con', 'sentido...                  other   \n",
      "3    ['i', 'love', 'people', 'with', 'a', 'large', ...                  other   \n",
      "4    ['have', 'courage', '&', 'be', 'kind', '.', 'w...                  other   \n",
      "..                                                 ...                    ...   \n",
      "705  ['the', 'sun', 'never', 'say', 'to', 'earth', ...                  other   \n",
      "706                                                NaN                    NaN   \n",
      "707                                                NaN                    NaN   \n",
      "708                                                NaN                    NaN   \n",
      "709                                                NaN                    NaN   \n",
      "\n",
      "    acad_prob  gov_prob  media_prob other_prob tourbiz_prob  Label  Label.Type  \n",
      "0         NaN       NaN         NaN        NaN          NaN  other        Hand  \n",
      "1    0.000776  0.000603    0.004676   0.993783     0.000162    NaN         NaN  \n",
      "2    0.009851  0.001305    0.006343   0.981409     0.001092    NaN         NaN  \n",
      "3    0.003543  0.000746    0.004899   0.990260     0.000552    NaN         NaN  \n",
      "4    0.003940  0.000537    0.007693   0.985011     0.002819    NaN         NaN  \n",
      "..        ...       ...         ...        ...          ...    ...         ...  \n",
      "705  0.002275  0.001970    0.036049   0.957599     0.002107    NaN         NaN  \n",
      "706       NaN       NaN         NaN        NaN          NaN  other        Hand  \n",
      "707       NaN       NaN         NaN        NaN          NaN  other        Hand  \n",
      "708       NaN       NaN         NaN        NaN          NaN  other     Predict  \n",
      "709       NaN       NaN         NaN        NaN          NaN  other     Predict  \n",
      "\n",
      "[710 rows x 42 columns]\n"
     ]
    }
   ],
   "source": [
    "# adding the account labels to the csv with tweets in order to accesss during tf-idf\n",
    "oil_w_acc = pd.merge(oil, accounts_merged, on='username', how='left')\n",
    "print(oil_w_acc)\n",
    "industrial_w_acc = pd.merge(industrial, accounts_merged, on='username', how='left')\n",
    "sewage_w_acc = pd.merge(sewage, accounts_merged, on='username', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "441577bb-0d96-4a2b-a485-2cdb081ea069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordcloud analysis based on account type\n",
    "\n",
    "#preprocessing\n",
    "oil_w_acc['text_with_display_links'].fillna('', inplace=True)\n",
    "industrial_w_acc['text_with_display_links'].fillna('', inplace=True)\n",
    "sewage_w_acc['text_with_display_links'].fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b43c3288-343c-4479-8cf8-cff8eccc9206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing for tf-idf: columns -> account type, word, n (number of accounts that mentioned that word)\n",
    "oil_acad = oil_w_acc[oil_w_acc[\"hand.label_simplified\"]==\"acad\"]\n",
    "word_counts = oil_acad.groupby(['username', 'text_with_display_links']).size().reset_index(name='count')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4a2ff38f-f817-4e8f-823c-f552fecb7569",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts['words'] = word_counts['text_with_display_links'].str.split()\n",
    "\n",
    "# Exploding the words column to create rows for each word\n",
    "word_counts = word_counts.explode('words')\n",
    "\n",
    "# Drop duplicates of user and words to get unique user-word pairs\n",
    "unique_user_word_pairs = word_counts[['username', 'words']].drop_duplicates()\n",
    "\n",
    "# Count unique users per word\n",
    "word_usage_count = unique_user_word_pairs.groupby('words')['username'].nunique().reset_index(name='count')\n",
    "\n",
    "# Count how many unique words have been used by at least one user\n",
    "unique_word_count = word_usage_count['words'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3141ea97-2da3-4c6e-9450-fe5a9d01f50c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "175"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0bb7814a-d0b4-4cfe-b942-4fddc4c850a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>oil</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>spill</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>tampa</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>science</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>gulf</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>morning</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>ecology</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>ecosystems</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>ellen</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>kill</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>175 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          words  count\n",
       "103         oil     22\n",
       "151       spill     22\n",
       "156       tampa     17\n",
       "144     science     14\n",
       "72         gulf     13\n",
       "..          ...    ...\n",
       "100     morning      1\n",
       "44      ecology      1\n",
       "46   ecosystems      1\n",
       "48        ellen      1\n",
       "87         kill      1\n",
       "\n",
       "[175 rows x 2 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_usage_count.sort_values(by='count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c1cec4-cc0d-4158-98a1-c07e323f9a82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "e37f6f24e9a9f73ff6720cc463cd31ad7ef16f22c85d3da331e44c0f6e80d360"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
